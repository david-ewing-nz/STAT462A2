---
title:    "STAT462 Assignment 2"
author: 
  -       "David Ewing (82171165)"
  -       "Xia Yu      (62380486)"
date:     "`r Sys.Date()`"
output:   
  pdf_document:
    latex_engine: xelatex
    fig_caption:     true
    number_sections: true
    toc:             true
    keep_tex:        false
    fig_crop: false         # disable pdfcrop 
fontsize: 11pt
geometry: margin=1in
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(dev.args = list(png = list(type = "cairo")))  # Summer I need this for my configuration of VS Code. Let me know if you need it removed. 

# Load libraries
library(tidyverse)
library(readr)
library(dplyr)
library(purrr)
library(tidyr)
library(flextable)
library(caret)


# ---- Pretty Summary Table for Numeric Features ----
pretty_summary <- \(df) {
  df %>%
    select(where(is.numeric)) %>%
    summarise(
      across(
        everything(),
        .fns = list(
          Mean     = \(x) mean(x, na.rm = TRUE),
          Median   = \(x) median(x, na.rm = TRUE),
          Min      = \(x) min(x, na.rm = TRUE),
          Max      = \(x) max(x, na.rm = TRUE),
          IQR      = \(x) IQR(x, na.rm = TRUE),
          NA_Count = \(x) sum(is.na(x))
        )
      )
    ) %>%
    pivot_longer(
      cols = everything(),
      names_to = c("Variable", ".value"),
      names_sep = "_"
    ) %>%
    mutate(across(where(is.numeric), round, 2)) %>%
    flextable() %>%
    set_header_labels(
      Variable = "Feature",
      Mean     = "Mean",
      Median   = "Median",
      Min      = "Min",
      Max      = "Max",
      IQR      = "Interquartile Range",
      NA_Count = "Missing Values"
    ) %>%
    autofit() %>%
    theme_vanilla()
}


# ---- Pretty CSV Loader with Type Summary and Preview ----
pretty_read_csv <- \(path, n = 0) {
  df <- readr::read_csv(path, show_col_types = FALSE)
  types <- purrr::map_chr(df, typeof)
  type_df <- data.frame(Column = names(df), Type = types, stringsAsFactors = FALSE)

  print(flextable::flextable(type_df) |> 
          flextable::autofit() |> 
          flextable::theme_vanilla())

  if (n > 0) print(utils::head(df, n))
  else if (n < 0) print(utils::tail(df, abs(n)))

  invisible(df)
}
```


---
title:    "STAT462 Assignment 2"
author: 
  -       "David Ewing (82171165)"
  -       "Xia Yu      (62380486)"
date:     "`r Sys.Date()`"
output:   
  pdf_document:
    fig_caption:     true
    number_sections: true
    toc:             true
    keep_tex:        false
fontsize: 11pt
geometry: margin=1in
---




## 1. Introduction

**Assignment 2 of STAT462** - The analyses are based on two datasets contained in a provided ZIP archive. All data preparation steps, statistical analyses, and plots follow the instructions in the assignment brief.

## 2. Data Extraction

Extract both datasets from the ZIP file located in the project’s `data/` directory.

```{r unzip-data}

# Data Extraction 
# 1 - Define path to the zip file and target extraction directory
# 2 - Get list of filenames in the zip
# 3 - Construct full paths for target files
# 4 - Create target directory if it doesn't exist
# 5 - Unzip only if any files are missing

zip_path  <- "../data/data_assignment_2.zip"
unzip_dir <- "../data/unzipped"
csv_files <- unzip(zip_path, list = TRUE)$Name # List contents
csv_files 
target_paths <- file.path(unzip_dir, csv_files)
if (!dir.exists(unzip_dir)) dir.create(unzip_dir, recursive = TRUE)
if (!all(file.exists(target_paths))) {
  unzip(zip_path, files = csv_files, exdir = unzip_dir)
}

```

## 3. Data Preparation

Read the extracted datasets and examine their structure.

```{r read-data}
colors_df <- read_csv(file.path(unzip_dir, "colors_train.csv"))
heart_df  <- read_csv(file.path(unzip_dir, "heart.csv"))

cat("==== Schema for", basename(target_paths[1]), "====\n")
glimpse(colors_df)
glimpse(heart_df)
```



```{r plot-colours-bar, eval=TRUE}
# Confirm available columns
print(names(colors_df))

# Plot colour distribution using the correct column
ggplot(colors_df, aes(x = color)) +
  geom_bar(fill = "steelblue") +
  labs(title = "Distribution of Colour Labels", x = "Colour", y = "Count") +
  theme_minimal()
```





## 4. Exploratory Data Analysis

We begin with summary statistics and visualise key variables from each dataset.

```{r eda}
# Summary statistics
summary(colors_df)
summary(heart_df)

# Example plots
ggplot(colors_df, aes(x = color)) +
  geom_bar() +
  labs(title = "Distribution of Colour Labels")

ggplot(heart_df, aes(x = CVD)) +
  geom_bar() +
  labs(title = "Heart Disease Diagnosis Frequencies")
```

## 5. Modelling

A logistic regression model is fit to the heart disease dataset.

```{r model}
# Fit logistic regression

# Create training and validation sets from heart_df
# This is a guess — please confirm if validation set should be constructed differently
set.seed(82171165)
train_idx <- createDataPartition(heart_df$CVD, p = 0.8, list = FALSE)
heart_train <- heart_df[train_idx, ]
heart_validate <- heart_df[-train_idx, ]


# this is a guess
model <- glm(CVD ~ ., data = heart_train, family = binomial)
summary(model)
```

## 6. Evaluation

We evaluate the model’s predictive performance using a threshold of 0.5.

```{r evaluation}
# Make predictions and evaluate
# this is a guess
predicted <- ifelse(predict(model, newdata = heart_validate, type = "response")   > 0.5, 1, 0)
table(Predicted = predicted, Actual = heart_validate$CVD)

# Calculate accuracy
accuracy <- mean(predicted == heart_validate$CVD)
accuracy
```

## 7. Discussion

In this section, we discuss the model’s performance, its implications, and any limitations observed during the analysis. This includes reflection on class balance, variable significance, and the reliability of the predictions.

## 8. Conclusion

The report demonstrates appropriate handling of ZIP-based data, exploratory analysis, logistic regression modelling, and evaluation, in line with the expectations of STAT462 Assignment 2.