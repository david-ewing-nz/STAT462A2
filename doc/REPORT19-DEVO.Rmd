---
title:    "STAT462 Assignment 2"
author: 
  -       "David Ewing (82171165)"
  -       "Xia Yu      (62380486)"
date:     "`r Sys.Date()`"

output:   
  pdf_document:
    latex_engine:     xelatex
    fig_caption:      true
    number_sections:  true
    toc:              true
    keep_tex:         true
    fig_crop:         false         # disable pdfcrop 
    includes:
      in_header: ../doc/fonts.tex
    
  mainfont: Arial
  fontsize: 10pt
---

```{r file-info, echo=FALSE, results='asis'}

cat("**R Markdown file name:**", knitr::current_input(), "\n\n")
cat("**R Markdown file name:**", rmarkdown::metadata$file, "\n")
cat("**Rendered on:**", format(Sys.time(), "%Y.%m.%d %H:%M:00"), "\n")
```

```{r setup, include=FALSE}
# This chunk was set up with the aid of ChatGPT.
# The intent is to load updates quietly thus not
# spending undue time with the logistics of getting 
# setup. 
 

#---------------- knitr::opts_chunk ---------------------------
#---------------- knitr::opts_chunk ---------------------------

knitr::opts_chunk$set(
  dev.args = list(
    png = list(type = "cairo")
    ),
  results = "markup",   # default("markup") "asis" "hide"	"hold"	"show"
  echo    = TRUE,      # Whether the code is shown
  eval    = TRUE,       # Whether the code is executed
  message = FALSE,      # Whether messages are printed
  warning = FALSE,     # Whether warnings are printed
  error   = FALSE       # Whether errors stop execution
  )


#---------------- loading libraries ---------------------------
#---------------- loading libraries ---------------------------

# Load configuration for reproducibility and preferred CRAN mirror
options(repos = c(CRAN = "https://cran.stat.auckland.ac.nz/"))

#library(conflicted) # before any other

# Required packages
required_packages <- c(
  "caret",         # Model training utilities
  "class",         # kNN 
  "cowplot",       # Plot composition
  "dplyr",         # Data wrangling
  "flextable",     # Summary tables
  "GGally",        # Pair plots
  "ggplot2",       # Core plotting
  "glmnet",        # Regularised regression
  "glue",          # String interpolation
  "kableExtra",    # Table formatting
  "knitr",         # Inline rendering
  "MASS",          # LDA/QDA and logistic
  "nnet",          # Neural networks (multinomial logistic regression)
  "officer",       # Word/PDF table styling (used by flextable)
  "patchwork",     # for ggplot grid layouts
 # "purr",
  "rsample",       # Data splitting
  "skimr",         # Data summaries
  "tree",          # Decision trees
  "yardstick",     # tidy classification metrics
  "tibble",        # Modern data frames
  "tidyverse"      # Core data science packages (optional/redundant)
)

for (pkg in required_packages) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
  }
  library(pkg, character.only = TRUE)
}

#---------------- conflict_prefer ------------------------------
#---------------- conflict_prefer ------------------------------
library(conflicted)
if ("conflicted" %in% loadedNamespaces()) {
  try(conflict_prefer("filter", "dplyr"), silent = TRUE)
  try(conflict_prefer("select", "dplyr"), silent = TRUE)
}

```


```{r helper-functions, include=FALSE}

#' Format a data frame as a styled flextable
pretty_df <- function(df,
                      title    = NULL,
                      fontsize = 10,
                      padding  = 5,  # defined here
                      n        = 5) {
  if (!is.data.frame(df)) {
    stop("Input must be a data frame")
  }

  # Optional row slicing
  if (n > 0) df <- head(df, n)
  if (n < 0) df <- tail(df, abs(n))

  ft <- flextable::flextable(df) |>
    flextable::fontsize(size = fontsize, part = "all") |>
    flextable::padding(padding = padding, part = "all") |>
    flextable::align(align = "center", part = "all") |>
    flextable::theme_booktabs() |>
    flextable::bold(i = 1, part = "header") |>
    flextable::autofit()

  # Estimate and optionally adjust for long titles
  if (!is.null(title)) {
    estimated_char_width <- 0.07
    title_width <- nchar(title) * estimated_char_width

    if (!is.null(ft$body$colwidths)) {
      current_width <- sum(ft$body$colwidths, na.rm = TRUE)

      if (title_width > current_width && current_width > 0) {
        scale_factor <- title_width / current_width
        new_widths <- ft$body$colwidths * scale_factor

        for (j in seq_along(new_widths)) {
          ft <- flextable::width(ft, j = j, width = new_widths[j])
        }
      }
    }

    ft <- flextable::set_caption(ft, caption = title)
  }

  return(ft)
}


#' Format a confusion matrix as flextable
pretty_cmtx <- function(cmtx_tbl, title = "Confusion Matrix") {
  df <- as.data.frame.matrix(cmtx_tbl)               # Convert to data.frame (wide format)
  df <- cbind(Predicted = rownames(df), df)      # Add row names as column
  rownames(df) <- NULL                           # Remove rownames
  pretty_df(df, title = title)                   # Format with your pretty_df()
}


#' Format classification metrics
pretty_heart_mtrx <- function(cmtx_tbl, title = "Classification Metrics") {
  metrics_df <- eval_heart_cmtx(cmtx_tbl)      # Compute the metrics
  pretty_df(metrics_df, title = title)      # Format as flextable
}



#' Summarise train/test splits
pretty_split <- function(train, test, target, title = "Train/Test Split Summary") {
  s <- function(data) {
    tbl <- table(data[[target]])
    out <- data.frame(
      Total = nrow(data),
      Class_1 = tbl[1],
      Class_2 = tbl[2],
      Prop_1 = round(tbl[1] / sum(tbl), 3),
      Prop_2 = round(tbl[2] / sum(tbl), 3)
    )
    rownames(out) <- NULL
    out
  }
  summary_df <- rbind(
    cbind(Set = "Train", s(train)),
    cbind(Set = "Test", s(test))
  )
  pretty_df(summary_df, title = title)
}


# ---- Pretty csv_read with Type Summary and Preview ------
pretty_read_csv <- \(path, n = 5, col_names = TRUE, show_col_types = FALSE) {
  df <- readr::read_csv(path, show_col_types = FALSE, col_names = col_names)
  df <- as.data.frame(df)
  ft <- pretty_df(df, glue::glue("CSV: {path}"))
  return(list(df = df, ft = ft))
}


# ---- Pretty Excel Reader with Type Summary and Preview ----
pretty_read_xlsx <- \(path, sheet = 1, col_names = TRUE, n = 0) {
  df <- readxl::read_excel(path, sheet = sheet, col_names = col_names)
  df <- as.data.frame(df)
  types <- purrr::map_chr(df, typeof)
  type_df <- data.frame(Column = names(df), Type = types, stringsAsFactors = FALSE)
  ft <- pretty_df(type_df, glue::glue("XLSX Column Types: {path}"))
  return(list(df = df, ft = ft))
}


# ---- Pretty ggplot metadata summary ----
pretty_ggplot <- function(plot, title = "ggplot Summary") {
  if (!inherits(plot, "gg")) stop("Input must be a ggplot object.")

  plot_data <- tryCatch(plot$data, error = function(e) NULL)
  geoms <- sapply(plot$layers, function(layer) class(layer$geom)[1])
  mappings <- plot$mapping

  global_aes  <- names(mappings)
  global_vals <- sapply(mappings, function(x) rlang::expr_text(x))

  p_title <- plot$labels$title %||% title %||% ""
  x_lab   <- plot$labels$x %||% ""
  y_lab   <- plot$labels$y %||% ""
  colour_lab <- plot$labels$colour %||% plot$labels$color %||% ""

  df <- data.frame(
    Component = c("Title", "X Axis", "Y Axis", "Colour Legend", "Geoms", global_aes),
    Value     = c(p_title, x_lab, y_lab, colour_lab, paste(geoms, collapse = ","), global_vals),
    stringsAsFactors = FALSE
  )
  pretty_df(df)
}


#' Pretty glm model summary
pretty_glm <- function(model, title = "GLM Model Summary" ) {
  if (!inherits(model, "glm")) {
    stop("pretty_glm() expects a glm object.")
  }
  tryCatch({
    df <- data.frame(
      Metric = c(
        "Formula", "AIC", "Null deviance", "Residual deviance",
        "Component names", "Model class"
      ),
      Value = c(
        deparse(formula(model)),
        AIC(model),
        model$null.deviance,
        model$deviance,
        paste(names(model), collapse = ", "),
        paste(class(model), collapse = ", ")
      ),
      stringsAsFactors = FALSE
    )
    ft <- pretty_df(df, title = "GLM Model Summary")
  }, error = function(e) {
    cat("Error in pretty_glm():", conditionMessage(e), "\n")
    NULL
  })
  return(ft)
}

pretty_multinom <- function(model, title = "Multinomial Model Summary") {
  if (!inherits(model, "multinom")) {
    stop("pretty_multinom() expects a multinom object.")
  }
#browser()
    #print(paste(deparse(formula(model)), collapse = ""))  
    #print(AIC(model))
    #print(model$converged)
  
    #Print(paste(class(model), collapse = ", ")) 
    #print(paste(names(model), collapse = ", "))
  tryCatch({
    # Summary table
    df_summary <- data.frame(
  Metric = c(
    "Formula", "AIC", "Converged", "Number of Iterations",
    "Model class", "Component names"
  ),
  Value = c(
    paste(deparse(formula(model)), collapse = ""),
    AIC(model),
    if (!is.null(model$converged)) model$converged else "N/A",
    if (!is.null(model$iters)) model$iters else "N/A",
    paste(class(model), collapse = ", "),
    paste(names(model), collapse = ", ")
  ),
  stringsAsFactors = FALSE
)


    ft_summary <- pretty_df(df_summary, title = title, n = nrow(df_summary))

    # Coefficients table
    coef_mat <- coef(model)
    coef_df <- as.data.frame(
      apply(coef_mat, 2, function(x) formatC(x, digits = 3, format = "e")),
      stringsAsFactors = FALSE
    )
    coef_df$Class <- rownames(coef_mat)
    coef_df <- coef_df[, c("Class", setdiff(names(coef_df), "Class"))]

    ft_coef <- pretty_df(coef_df, title = "Multinomial Model Coefficients")

    return(list(ft_summary, ft_coef))
  }, error = function(e) {
    cat("Error in pretty_multinom():", conditionMessage(e), "\n")
    NULL
  })
}



#' Pretty split df into multiple flextables
pretty_split_df <- function(df,
                            cols = 6,
                            title = NULL,
                            fontsize = 10,
                            n = 5) {
  if (!is.data.frame(df)) {
    stop(cat("\\textit{Object is not a data frame:}", deparse(df)))
  }

  title <- if (is.null(title)) deparse(substitute(df)) else title
  df_show <- if (n > 0) head(df, n) else if (n < 0) tail(df, abs(n)) else df
  col_groups <- split(names(df_show), ceiling(seq_along(df_show) / cols))

  ft_list <- lapply(seq_along(col_groups), function(i) {
    subdf <- df_show[, col_groups[[i]], drop = FALSE]
    pretty_df(
      subdf,
      title = paste0(title, " (", i, ")"),
      fontsize = fontsize,
      n = n
    )
  })

  names(ft_list) <- paste0(
    title,
    " (",
    seq_along(ft_list),
    ")"
  )

  return(ft_list)
}


#' Generate a full QDA summary with priors, centroids, and equations
pretty_qda <- function(qda_model) {
  response_var <- as.character(attr(qda_model$terms, "variables")[[2]])
  predictor_vars <- attr(qda_model$terms, "term.labels")

  qda_equation <- paste0(
    "P(", response_var, " = k | ", paste(predictor_vars, collapse = ", "),
    ") ∝ π_k × f_k(", paste(predictor_vars, collapse = ", "), ")"
  )

  centroids_df <- as.data.frame(qda_model$means)
  centroids_df[[response_var]] <- rownames(centroids_df)
  centroids_df <- centroids_df[, c(response_var, setdiff(names(centroids_df), response_var))]

  priors_df <- as.data.frame(qda_model$prior)
  priors_df[[response_var]] <- rownames(priors_df)
  colnames(priors_df) <- c("prior_probability", response_var)
  priors_df <- priors_df[, c(response_var, "prior_probability")]

  return(list(
    centroids = pretty_df(centroids_df,qda_equation),
    priors    = priors_df,pretty_df(priors_df,qda_equation),
    equation = qda_equation
  ))
}


#' Format classification thresholds summary
pretty_thresh <- function(predicted_probs, truth, threshold = 0.5, positive_class = NULL, title = "Threshold-Based Classification Summary") {
  predicted_class <- ifelse(predicted_probs >= threshold, positive_class, paste0("not_", positive_class))
  cm <- table(Truth = truth, Predicted = predicted_class)
  pretty_cmtx(cm, title = title)
}


#' Summarise variable transformations
pretty_transforms <- function(transform_map, title = "Variable Transformation Mapping") {
  df <- as.data.frame(transform_map)
  names(df) <- c("Original", "Transformed")
  pretty_df(df, title = title)
}


#' Summarise missing value patterns
pretty_missing <- function(df, title = "Missing Data Summary") {
  missings <- sapply(df, function(x) sum(is.na(x)))
  df_out <- data.frame(Variable = names(missings), Missing_Count = missings)
  df_out <- df_out[df_out$Missing_Count > 0, ]
  pretty_df(df_out, title = title)
}


#' Display model object summary (generic use for tree, lm, etc.)
pretty_model <- function(model, title = "Model Summary") {
  summary_text <- capture.output(summary(model))
  df <- data.frame(Summary = summary_text)
  pretty_df(df, title = title)
}


 


#' Pretty summary for numeric columns
pretty_summary <- function(df) {
  ft <- dplyr::select(df, where(is.numeric)) |>
    dplyr::summarise(
      dplyr::across(
        dplyr::everything(),
        .fns = list(
          Mean     = ~mean(.x, na.rm = TRUE),
          Median   = ~median(.x, na.rm = TRUE),
          Min      = ~min(.x, na.rm = TRUE),
          Max      = ~max(.x, na.rm = TRUE),
          IQR      = ~IQR(.x, na.rm = TRUE),
          nNA      = ~sum(is.na(.x))
        )
      )
    ) |>
    tidyr::pivot_longer(
      cols = dplyr::everything(),
      names_to = c("Variable", ".value"),
      names_sep = "_"
    ) |>
    dplyr::mutate(dplyr::across(where(is.numeric), round, 2)) |>
    flextable::flextable() |>
    flextable::set_header_labels(
      Variable = "Feature",
      Mean     = "Mean",
      Median   = "Median",
      Min      = "Min",
      Max      = "Max",
      IQR      = "Interquartile Range",
      nNA      = "Missing Values"
    ) |>
    flextable::autofit() |>
    flextable::theme_vanilla()

  ft
}

pretty_heart_comparison <- function(model_list) {
  stopifnot(is.list(model_list))
  
  # Storage
  comparison <- data.frame()
  
  for (name in names(model_list)) {
    entry <- model_list[[name]]
    
    # Required elements in each entry
    actual <- entry$actual
    predicted <- entry$predicted
    
    # Generate confusion matrix
    cm <- make_heart_cmtx(predicted, actual)
    metrics <- eval_heart_cmtx(cm)
    
    comparison <- rbind(comparison, data.frame(
      Model        = name,
      Accuracy     = round(metrics[2], 3),
      Precision    = round(metrics[3], 3),
      Recall       = round(metrics[4], 3),
      F1_Score     = round(metrics[5], 3),
      MisclassRate = round(metrics[1], 3)
    ))
  }
  
  # Reorder columns to preference
  comparison <- comparison[, c("Model", "Accuracy", "Precision", "Recall", "F1_Score", "MisclassRate")]
  
  # Return as pretty flextable
  pretty_df(comparison, title = "Model Performance Comparison")
  
  
  
}





```



```{r wsb-global-model-utils}
# --- Model Evaluation Utilities ---

model_boolean_eval <- function(model, pred_class, actual_class, train_data, threshold = 0.5) {
  # Ensure inputs are numeric 0/1
  actual_class <- if (is.factor(actual_class)) as.integer(actual_class) - 1 else actual_class
  pred_class   <- if (is.factor(pred_class)) as.integer(pred_class) - 1 else pred_class
  train_death  <- if (is.factor(train_data$DEATH)) as.integer(train_data$DEATH) - 1 else train_data$DEATH

  # Significance test
  pvals <- broom::tidy(model)$p.value
  sig_all <- all(pvals < 0.05)

  # Accuracy comparisons
  test_acc <- mean(pred_class == actual_class)
  base_acc <- max(prop.table(table(actual_class)))
  better_than_random <- test_acc > 0.5

  better_than_baseline_mild     <- (test_acc - base_acc) >= 0.01
  better_than_baseline_moderate <- (test_acc - base_acc) >= 0.015
  better_than_baseline_strong   <- (test_acc - base_acc) >= 0.02
  real_world_gain               <- (test_acc - base_acc) >= 0.05

  # Generalisation test
  train_probs <- predict(model, newdata = train_data, type = "response")
  train_class <- ifelse(train_probs > threshold, 1, 0)
  train_acc <- mean(train_class == train_death)
  well_generalised <- abs(train_acc - test_acc) < 0.05

  list(
    sig_all = sig_all,
    test_acc = test_acc,
    base_acc = base_acc,
    better_than_random = better_than_random,
    better_than_mild = better_than_baseline_mild,
    better_than_moderate = better_than_baseline_moderate,
    better_than_strong = better_than_baseline_strong,
    well_generalised = well_generalised,
    real_world_gain = real_world_gain
  )
}

model_multiclass_eval <- function(pred_class, actual_class) {
  # Ensure inputs are factors with correct levels
  labels <- c("RED", "GREEN", "BLUE")
  pred_class   <- if (is.numeric(pred_class)) factor(pred_class, levels = 1:3, labels = labels) else pred_class
  actual_class <- if (is.numeric(actual_class)) factor(actual_class, levels = 1:3, labels = labels) else actual_class

  # Accuracy and baseline
  accuracy <- mean(pred_class == actual_class)
  base_acc <- max(prop.table(table(actual_class)))
  better_than_baseline <- (accuracy - base_acc) >= 0.02
  strong_performance   <- accuracy >= 0.75

  # Macro-averaged precision and recall
  precision <- recall <- rep(NA, 3)
  for (i in seq_along(labels)) {
    class <- labels[i]
    tp <- sum(pred_class == class & actual_class == class)
    fp <- sum(pred_class == class & actual_class != class)
    fn <- sum(pred_class != class & actual_class == class)
    precision[i] <- if ((tp + fp) > 0) tp / (tp + fp) else NA
    recall[i]    <- if ((tp + fn) > 0) tp / (tp + fn) else NA
  }

  macro_precision <- mean(precision, na.rm = TRUE)
  macro_recall    <- mean(recall, na.rm = TRUE)

  list(
    accuracy = accuracy,
    base_acc = base_acc,
    better_than_baseline = better_than_baseline,
    strong_performance = strong_performance,
    macro_precision = macro_precision,
    macro_recall = macro_recall
  )
}

summarise_boolean_eval <- function(eval_list) {
  data.frame(
    Question = c(
      "Statistically significant?",
      "Better than random?",
      "Better than majority guess? (≥ 0.01)",
      "Better than majority guess? (≥ 0.015)",
      "Better than majority guess? (≥ 0.02)",
      "Well-generalised?",
      "Real-world useful?"
    ),
    Result = unlist(c(
      eval_list$sig_all,
      eval_list$better_than_random,
      eval_list$better_than_mild,
      eval_list$better_than_moderate,
      eval_list$better_than_strong,
      eval_list$well_generalised,
      eval_list$real_world_gain
    )),
    Justification = c(
      "all(pvals < 0.05)",
      "test_acc > 0.5",
      "(test_acc - base_acc) >= 0.01",
      "(test_acc - base_acc) >= 0.015",
      "(test_acc - base_acc) >= 0.02",
      "abs(train_acc - test_acc) < 0.05",
      "(test_acc - base_acc) >= 0.05"
    ),
    stringsAsFactors = FALSE
  )
}



#---------------- prediction & confusion helpers ------------------------------


predict_heart_classes <- function(probs, threshold = 0.5) {
  pred <- ifelse(probs > threshold, 1, 0)
  factor(pred, levels = c(0, 1), labels = c("LIVE", "DEAD")) #binary probabilities to factor labels for heart
}

# Create confusion matrix for heart
make_heart_cmtx <- function(pred_class, actual_class) {
  pred_class <- if (is.numeric(pred_class)) {
    factor(pred_class, levels = c(0, 1), labels = c("LIVE", "DEAD"))
  } else pred_class
  actual_class <- if (is.numeric(actual_class)) {
    factor(actual_class, levels = c(0, 1), labels = c("LIVE", "DEAD"))
  } else actual_class

  table(
    Predicted = factor(pred_class, levels = c("LIVE", "DEAD")),
    Actual    = factor(actual_class, levels = c("LIVE", "DEAD"))
  )
}

eval_heart_cmtx <- function(cmtx_tbl) {
  if (!(all(rownames(cmtx_tbl) == c("LIVE", "DEAD")) || !all(colnames(cmtx_tbl) == c("LIVE", "DEAD")))) {
    stop("Confusion matrix must use levels: LIVE and DEAD in [row, col] format")
  }

  tp <- cmtx_tbl["DEAD", "DEAD"]
  tn <- cmtx_tbl["LIVE", "LIVE"]
  fp <- cmtx_tbl["LIVE", "DEAD"]
  fn <- cmtx_tbl["DEAD", "LIVE"]
  total <- sum(cmtx_tbl)

  if (total == 0 || any(is.na(c(tp, tn, fp, fn)))) {
    return(data.frame(
      Metric = c("Misclass Rate", "Accuracy", "Precision", "Recall", "F1 Score"),
      Value = rep(NA_real_, 5)
    ))
  }

  accuracy  <- (tp + tn) / total
  precision <- if ((tp + fp) > 0) tp / (tp + fp) else NA
  recall    <- if ((tp + fn) > 0) tp / (tp + fn) else NA
  f1        <- if (!is.na(precision) && !is.na(recall) && (precision + recall) > 0) {
    2 * precision * recall / (precision + recall)
  } else NA
  misclass  <- 1 - accuracy

  data.frame(
    Metric = c("Misclass Rate", "Accuracy", "Precision", "Recall", "F1 Score"),
    Value  = round(c(misclass, accuracy, precision, recall, f1), 3)
  )
}



summarise_heart_results <- function(pred_probs, actual_class, threshold = 0.5) {
  pred_class <- predict_heart_classes(pred_probs, threshold)
  cmtx_tbl   <- make_heart_cmtx(pred_class, actual_class)
  metrics_df <- eval_heart_cmtx(cmtx_tbl)

  list( 
    pred_class = pred_class,
    cmtx_tbl   = cmtx_tbl,                     # raw
    metrics_df = metrics_df,                   # raw
    cmtx_ft    = pretty_cmtx(cmtx_tbl),
    mtrx_ft    = pretty_df(metrics_df)
  )
}




# Convert multiclass probabilities to factor labels for colour
predict_colour_classes <- function(probs) {
  pred <- apply(probs, 1, which.max)
  factor(pred, levels = 1:3, labels = c("RED", "GREEN", "BLUE"))
}

# Create confusion matrix for colour
make_colour_cmtx <- function(pred_class, actual_class) {
  pred_class <- if (is.numeric(pred_class)) factor(pred_class, levels = 1:3, labels = c("RED", "GREEN", "BLUE")) else pred_class
  actual_class <- if (is.numeric(actual_class)) factor(actual_class, levels = 1:3, labels = c("RED", "GREEN", "BLUE")) else actual_class
  table(
    Predicted = factor(pred_class, levels = c("RED", "GREEN", "BLUE")),
    Actual    = factor(actual_class, levels = c("RED", "GREEN", "BLUE"))
  )
}

eval_colour_cmtx <- function(cmtx) {
  expected_levels <- c("RED", "GREEN", "BLUE")
  if (!all(rownames(cmtx) == expected_levels) || !all(colnames(cmtx) == expected_levels)) {
    stop("Confusion matrix must use levels: RED, GREEN, BLUE in [row, col] format")
  }

  # Total correct predictions
  correct <- sum(diag(cmtx))
  total   <- sum(cmtx)
  accuracy <- correct / total

  # Precision, Recall, F1 per class
  precision_vec <- recall_vec <- f1_vec <- numeric(length(expected_levels))

  for (i in seq_along(expected_levels)) {
    cls <- expected_levels[i]
    tp  <- cmtx[cls, cls]
    fp  <- sum(cmtx[cls, ]) - tp
    fn  <- sum(cmtx[, cls]) - tp

    precision <- if ((tp + fp) > 0) tp / (tp + fp) else NA
    recall    <- if ((tp + fn) > 0) tp / (tp + fn) else NA
    f1        <- if (!is.na(precision) && !is.na(recall) && (precision + recall) > 0) {
                   2 * precision * recall / (precision + recall)
                 } else NA

    precision_vec[i] <- precision
    recall_vec[i]    <- recall
    f1_vec[i]        <- f1
  }

  macro_precision <- mean(precision_vec, na.rm = TRUE)
  macro_recall    <- mean(recall_vec, na.rm = TRUE)
  macro_f1        <- mean(f1_vec, na.rm = TRUE)
  misclass        <- 1 - accuracy

  data.frame(
    Metric = c("Misclass Rate", "Accuracy", "Precision (macro)", "Recall (macro)", "F1 Score (macro)"),
    Value  = round(c(misclass, accuracy, macro_precision, macro_recall, macro_f1), 3)
  )
}


make_colour_cmtx <- function(pred_class, actual_class) {
  pred_class <- if (is.numeric(pred_class)) {
    factor(pred_class, levels = 1:3, labels = c("RED", "GREEN", "BLUE"))
  } else pred_class
  actual_class <- if (is.numeric(actual_class)) {
    factor(actual_class, levels = 1:3, labels = c("RED", "GREEN", "BLUE"))
  } else actual_class

  table(
    Predicted = factor(pred_class, levels = c("RED", "GREEN", "BLUE")),
    Actual    = factor(actual_class, levels = c("RED", "GREEN", "BLUE"))
  )
}

#' Fit GLM, QDA, and Tree models for heart 

fit_heart_models <- function(formula, train_data) {
  
  # formula    = the model formula (e.g., DEATH ~ GLUCOSE + SYSBP + AGE)
  # train_data = the training dataset (e.g., hp_train
  
  glm_model   <- glm(formula, data = train_data, family = binomial)
  qda_model   <- qda(formula, data = train_data)
  tree_model  <- tree(formula, data = train_data)
  
  list(
    formula       = formula,    # formula
    hp_model      = glm_model,  # GLM   equal classes linear and simple 
    hp_qda_model  = qda_model,  # QDA   class specific quadratic and spead out
    hp_tree_model = tree_model  # tree  human readable  
  )
}

#' Fit GLM, QDA, and Tree models for heart analysis
#' @param formula the model formula (e.g., DEATH ~ GLUCOSE + SYSBP + AGE)
#' @param train_data the training dataset (e.g., hp_train)
#' @return a named list of models and the equation used

fit_heart_models <- function(formula, train_data) {
  glm_model   <- glm(formula, data = train_data, family = binomial)
  qda_model   <- qda(formula, data = train_data)
  tree_model  <- tree(formula, data = train_data)
  
  list(
    formula     = formula,
    hp_model    = glm_model,
    hp_qda_model = qda_model,
    hp_tree_model = tree_model
  )
}




#' Eval heart model at   threshold , return full output set

# results_<LM-TYPE_<THRESHOLD> 
#  │
#  ├── pred_class         # Factor: predicted labels ("LIVE", "DEAD")
#  ├── cmtx_tbl           # Table: confusion matrix
#  ├── cmtx_ft            # Flextable: formatted confusion matrix
#  ├── metrics_df         # Data frame of evaluation metrics
#  │   ├── Type           # Character: model type, e.g. "GLM"
#  │   ├── Threshold      # Character or numeric: threshold used, e.g. 0.10
#  │   ├── Metric         # Character: metric name
#  │   │   ├── "Misclass Rate"
#  │   │   ├── "Accuracy"
#  │   │   ├── "Precision"
#  │   │   ├── "Recall"
#  │   │   └── "F1 Score"
#  │   └── Value          # Numeric: corresponding metric value (e.g., 0.336)
#  ├── metrics_ft         # Flextable: formatted version of metrics_df
#  └── misclass           # Numeric: extracted misclassification rate (e.g., 0.664)



#' 
run_heart_eval <- function(model, threshold, type = "GLM") {
  #  predicted probabilities
  probs <- switch(
    type,
    "TREE" = predict(model, newdata = hp_test, type = "vector")[, "DEAD"],
    "GLM"  = predict(model, newdata = hp_test, type = "response"),
    "QDA"  = predict(model, newdata = hp_test)$posterior[, "DEAD"],
    stop("Unsupported model type")
  )

  #   class labels and confusion matrix
  pred_class <- predict_heart_classes(probs, threshold)
  cmtx_tbl   <- make_heart_cmtx(pred_class, hp_actual_class)
  cmtx_title <- glue("Confusion Matrix: {type}, Threshold = {threshold}")
  cmtx_ft    <- pretty_cmtx(cmtx_tbl, cmtx_title)

  # EVAL metrics
  metrics_df <- eval_heart_cmtx(cmtx_tbl) |>
    mutate(
      Threshold = as.character(threshold),
      Type = type
    ) |>
    relocate(Threshold, .before = Metric)

  # format metrics table
  metrics_title <- glue("Performance: {type}, Threshold = {threshold}")
  metrics_ft    <- pretty_df(metrics_df, title = metrics_title)

  # Step 5: Extract misclassification rate
  misclass <- metrics_df$Value[metrics_df$Metric == "Misclass Rate"]

  # Step 6: Return all outputs
  list(
    pred_class  = pred_class,
    cmtx_tbl    = cmtx_tbl,
    cmtx_ft     = cmtx_ft,
    metrics_df  = metrics_df,
    metrics_ft  = metrics_ft,
    misclass    = misclass
  )
}


```





```{r load-and-test-split, results='hide'}
# Load heart and colour datasets from zip

# Set base directory
# Assign named paths based on filename match
# load the dataframes via pretty_read_csv 
# preview structure
unzip_dir <- "../data/unzipped"
zip_path  <- "../data/data_assignment_2.zip"
csv_files <- unzip(zip_path, list = TRUE)$Name  # Extract filenames from the zip
target_paths <- file.path(unzip_dir, csv_files)

idx_color   <- grep("color", csv_files)
idx_heart   <- grep("heart", csv_files)
path_colour <- target_paths[idx_color]
path_heart  <- target_paths[idx_heart]
 
heart_list  <- pretty_read_csv(path_heart, col_names = TRUE)
color_ft    <- pretty_read_csv(path_colour, col_names = TRUE)
   
heart_df = heart_list$df
heart_ft = heart_list$ft 
                                                         
color_df = color_ft$df
color_ft = color_ft$ft 


foo <- pretty_split_df(heart_df)

render_flextables <- function(ft_list) {
  for (ft in foo) {
    invisible(print(knitr::knit_print(ft)))
    }
  }

render_flextables(foo)
```
 


```{r heart-wsb-step-00-index, echo=FALSE}
# Define the index of evaluated heart models
heart_eval_index <- as.data.frame(
  tribble(
    ~Model,     ~Threshold, ~`Evaluation Label`,                          ~`Variable Name`,
    "GLM",       "0.10", "GLM at 0.10 threshold",           "results_glm_10",
    
    "GLM",       "0.25", "GLM at 0.25 threshold",           "results_glm_25",
     
    "GLM",       "0.50", "GLM at 0.50 threshold",             "results_glm_50",
    
    "GLM",       "0.75", "GLM at 0.75 threshold",            "results_glm_75",
    
    "GLM",       "0.90", "GLM at 0.90 threshold",              "results_glm_90",
    
    "QDA",       "0.50", "QDA at 0.50 threshold",              "results_qda_50",
    
    "Tree",      "0.50", "Tree at 0.50 threshold",              "results_tree_50",
    
    "GLM (ref)", "0.50", "GLM (reference model) for comparison", "results_glm_50"
  )
)
heart_output_usage <- tribble(
  ~`Output Element`, ~`Used For`,
  "metrics_df", "Comparisons, inline writeup, plots, export",
  "metrics_ft", "Report-ready display",
  "cmtx_tbl",   "If you want to calculate stats manually",
  "cmtx_ft",    "Report-ready display of confusion matrix",
  "misclass",    "Writeup commentary (\"the misclass rate was…\")",
  "pred_class",  "Plots, boundary visuals"
) |> 
  as.data.frame()

ft_heart_output_usage <- pretty_df(heart_output_usage, title = "Output Elements from run_heart_eval() and Their Use")
print(ft_heart_output_usage)

# Format as a flextable
ft_heart_eval_index <- pretty_df(heart_eval_index, title = "Index of Evaluated Heart Models")

# Print for early inspection
print(ft_heart_eval_index)

```


```{r wsb-step-03-split}
cat("### Split the dataset into a training set (80%) and a test set (20%)\n")

# Define conversion helpers
as_hp <- function(x) factor(x, levels = c(0, 1), labels = c("LIVE", "DEAD"))
as_h  <- function(x) as.integer(x) - 1

# Select and convert DEATH to binary early
h_vars   <- c("DEATH", "GLUCOSE", "SYSBP", "AGE")
h_clean  <- heart_df |>
  select(all_of(h_vars)) |>
  drop_na() |>
  mutate(DEATH = as_h(DEATH))  # 0/1 version for modeling

# Stratified split using binary DEATH
set.seed(82171165)
h_split <- initial_split(h_clean, prop = 0.8, strata = DEATH)
h_train <- training(h_split)
h_test  <- testing(h_split)

# Pretty/labeled version for visualization
hp_clean <- h_clean |> mutate(DEATH = as_hp(DEATH))
hp_train <- h_train |> mutate(DEATH = as_hp(DEATH))
hp_test  <- h_test  |> mutate(DEATH = as_hp(DEATH))

# True observed labels for evaluation
h_actual_class  <- h_test$DEATH
hp_actual_class <- hp_test$DEATH

```


```{r wsb-step-03-split}
cat("### Split the dataset into a training set (80%) and a test set (20%)
")

# heart_df exists at run time

h_vars <- c("DEATH", "GLUCOSE", "SYSBP", "AGE")
h_clean <- heart_df |>
  select(all_of(h_vars)) |>
  drop_na() |>
  mutate(DEATH = as.factor(DEATH))

# Stratified split : DEATH
set.seed(82171165)
h_split   <- initial_split(h_clean, prop = 0.8, strata = DEATH)
h_train   <- training(h_split)
h_test    <- testing(h_split)

# For visualisation only: relabel  "LIVE" and "DEAD"
levels = c(0, 1)
labels = c("LIVE", "DEAD")


hp_clean <- h_clean |>
  mutate(DEATH = factor(DEATH, levels = levels, labels = labels))
hp_train <- h_train |>
  mutate(DEATH = factor(DEATH, levels = levels, labels = labels))
hp_test <- h_test |>
  mutate(DEATH = factor(DEATH, levels = levels, labels = labels))

h_actual_class  <- h_test$DEATH        # true observed values
hp_actual_class <- hp_test$DEATH

```
### Visualise the relationship between DEATH, GLUCOSE and SYSBP
 
```{r wsb-step-04-visual}
cat("### Visualise the relationship between DEATH, GLUCOSE and SYSBP\n")

hp_relationship <- ggplot(hp_clean, aes(x = GLUCOSE, y = SYSBP, colour = DEATH)) +
  geom_point(alpha = 0.6) +
  labs(title = "DEATH vs GLUCOSE and SYSBP", x = "GLUCOSE", y = "SYSBP")

# 1. Distribution of SYSBP
hp_sysbp <- ggplot(hp_test, aes(x = SYSBP)) +
  geom_histogram(bins = 40, fill = "steelblue", colour = "white") +
  labs(title = "Distribution of SYSBP", x = "SYSBP", y = "Count")

# 2. Distribution of GLUCOSE
hp_glucose <- ggplot(hp_test, aes(x = GLUCOSE)) +
  geom_histogram(bins = 40, fill = "darkorange", colour = "white") +
  labs(title = "Distribution of GLUCOSE", x = "GLUCOSE", y = "Count")

# 3–6. Combination of linear/log axis scales
base <- ggplot(hp_test, aes(x = GLUCOSE, y = SYSBP)) +
  geom_point(alpha = 0.6) +
  labs(title = NULL, x = "GLUCOSE", y = "SYSBP")

hp_lin_lin <- base + ggtitle("Linear X / Linear Y")
hp_log_lin <- base + scale_x_log10() + ggtitle("Log X / Linear Y")
hp_lin_log <- base + scale_y_log10() + ggtitle("Linear X / Log Y")
hp_log_log <- base + scale_x_log10() + scale_y_log10() + ggtitle("Log X / Log Y")

# Combine 4 into a grid
hp_combined <- (hp_lin_lin | hp_log_lin) / (hp_lin_log | hp_log_log)

# Output all plots
print(hp_relationship)
print(hp_sysbp)
print(hp_glucose)
print(hp_combined)

pretty_ggplot(hp_relationship, title = "Relationship: DEATH vs GLUCOSE and SYSBP")
pretty_ggplot(hp_sysbp, title = "Distribution of SYSBP")
pretty_ggplot(hp_glucose, title = "Distribution of GLUCOSE")
pretty_ggplot(hp_lin_lin, title = "Linear X / Linear Y")
pretty_ggplot(hp_log_lin, title = "Log X / Linear Y")
pretty_ggplot(hp_lin_log, title = "Linear X / Log Y")
pretty_ggplot(hp_log_log, title = "Log X / Log Y")
pretty_ggplot(hp_combined, title = "Log/Linear Scale Comparison Grid")
```


### Form an initial hypothesis of what to look for when doing the classification

 ->
```{r  heart-wsb-step-03}
print("We hypothesise that high GLUCOSE and high SYSBP are associated with higher DEATH risk.")
```

### Fit a logistic regression model on the training set





























 
```{r  heart-wsb-step-04}
               
equation_lg <- DEATH ~ GLUCOSE + SYSBP + AGE
hp_model <- glm(equation_lg, data = hp_train, family = "binomial")
pretty_glm(hp_model)


```


```{r heart-step-fit-and-eval-for-one}
cat("### Fit Heart Models and Evaluate Logistic Regression (Threshold = 0.5)\n")


hp_equation_lg <- DEATH ~ GLUCOSE + SYSBP + AGE              #  model equation
heart_models   <- fit_heart_models(hp_equation_lg, hp_train) # models with  training data
hp_results_50  <- run_heart_eval(                            # eval t@ threshold 0.5
  model       = heart_models$hp_model,
  threshold   = 0.5 
)

print(hp_results_50$cmtx_ft)           # confusion matrix
print(hp_results_50$metrics_ft)        # metrics 
```


```{r heart-step-fit-and-eval-for-the-rest-of-them}
cat("### Run All Heart Model Evaluations\n")


hp_equation_lg <- DEATH ~ GLUCOSE + SYSBP + AGE
heart_models   <- fit_heart_models(hp_equation_lg, hp_train)

# GLM with multiple thresholds
results_GLM_10  <- run_heart_eval(heart_models$hp_model,       0.10)
results_GLM_25  <- run_heart_eval(heart_models$hp_model,       0.25)
results_GLM_50  <- run_heart_eval(heart_models$hp_model,       0.50) #**
results_GLM_75  <- run_heart_eval(heart_models$hp_model,       0.75)
results_GLM_90  <- run_heart_eval(heart_models$hp_model,       0.90)
results_QDA_50  <- run_heart_eval(heart_models$hp_qda_model,   0.50, "QDA")
results_TREE_50 <- run_heart_eval(heart_models$hp_tree_model,  0.50, "TREE")

```



```{r heart-wsb-step-15-threshold-comparison, echo=FALSE}
cat("### Threshold Comparison: Logistic Regression Classification Performance\n")

# Build a single summary data frame from all GLM threshold runs
heart_M_threshold_df <- bind_rows(
  results_GLM_10$metrics_df  |> mutate(Threshold = "0.25"),
  results_GLM_50$metrics_df  |> mutate(Threshold = "0.50"),
  results_GLM_75$metrics_df  |> mutate(Threshold = "0.75"),
  results_GLM_90$metrics_df  |> mutate(Threshold = "0.90"),
  results_GLM_90$metrics_df  |> mutate(Threshold = "0.90"),
  results_QDA_50$metrics_df  |> mutate(Threshold = "0.50"),
  results_TREE_50$metrics_df |> mutate(Threshold = "0.50")
) |>
  relocate(Threshold, .before = Metric) # make pretty

# Format as flextable
ft_M_threshold_summary  <- pretty_df(
  heart_M_threshold_df, 
  title = "Classification Performance Across Models and Thresholds"
  )
# Print
print(ft_M_threshold_summary )

```



```{r heart-wsb-step-17-metrics-and-ft-lists, include=FALSE}
cat("### Generate lists of heart model metrics and formatted flextables\n")

# create a list of metrics_df from each model-threshold result
heart_M_threshold_df <- list(
  GLM_10  = results_GLM_10$metrics_df,
  GLM_25  = results_GLM_25$metrics_df,
  GLM_50  = results_GLM_50$metrics_df,
  GLM_75  = results_GLM_75$metrics_df,
  GLM_90  = results_GLM_90$metrics_df,
  QDA_50  = results_QDA_50$metrics_df,
  TREE_50 = results_TREE_50$metrics_df
)

# create a a list of corresponding flextables with titles
heart_M_threshold_ft <- purrr::imap(
  heart_M_threshold_df,
  ~ pretty_df(.x, title = glue("{.y} Performance"))
)
```

 




