---
title:    "STAT462 Assignment 2"
author: 
  -       "David Ewing (82171165)"
  -       "Xia Yu      (62380486)"
date:     "`r Sys.Date()`"

output:   
  pdf_document:
    latex_engine:     xelatex
    fig_caption:      true
    number_sections:  true
    toc:              true
    keep_tex:         true
    fig_crop:         false         # disable pdfcrop 
    includes:
      in_header: ../doc/fonts.tex
    
  mainfont: Arial
  fontsize: 10pt
---

```{r file-info, echo=FALSE, results='asis'}

cat("**R Markdown file name:**", knitr::current_input(), "\n\n")
cat("**R Markdown file name:**", rmarkdown::metadata$file, "\n")
cat("**Rendered on:**", format(Sys.time(), "%Y.%m.%d %H:%M:00"), "\n")
```

```{r setup, include=FALSE}
# This chunk was set up with the aid of ChatGPT.
# The intent is to load updates quietly thus not
# spending undue time with the logistics of getting 
# setup. 
 

#---------------- knitr::opts_chunk ---------------------------
#---------------- knitr::opts_chunk ---------------------------

knitr::opts_chunk$set(
  dev.args = list(
    png = list(type = "cairo")
    ),
  results = "markup",   # default("markup") "asis" "hide"	"hold"	"show"
  echo    = TRUE,      # Whether the code is shown
  eval    = TRUE,       # Whether the code is executed
  message = FALSE,      # Whether messages are printed
  warning = FALSE,     # Whether warnings are printed
  error   = FALSE       # Whether errors stop execution
  )


#---------------- loading libraries ---------------------------
#---------------- loading libraries ---------------------------

# Load configuration for reproducibility and preferred CRAN mirror
options(repos = c(CRAN = "https://cran.stat.auckland.ac.nz/"))

#library(conflicted) # before any other

# Required packages
required_packages <- c(
  "caret",         # Model training utilities
  "class",         # kNN 
  "cowplot",       # Plot composition
  "dplyr",         # Data wrangling
  "flextable",     # Summary tables
  "GGally",        # Pair plots
  "ggplot2",       # Core plotting
  "glmnet",        # Regularised regression
  "glue",          # glue
  "patchwork",      # ✅ for ggplot grid layouts
  "rsample",         # initial_split
  "kableExtra",    # Table formatting
  "knitr",         # Inline rendering
  "MASS",          # LDA/QDA and logistic
  "nnet",
  "officer",       # Word/PDF table styling (used by flextable)
  "skimr",         # Data summaries
  "tree",
  "tidyverse",     # Core data science packages
  "tibble"         # Modern data frames
)
for (pkg in required_packages) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
  }
  library(pkg, character.only = TRUE)
}

#---------------- conflict_prefer ------------------------------
#---------------- conflict_prefer ------------------------------
library(conflicted)
if ("conflicted" %in% loadedNamespaces()) {
  try(conflict_prefer("filter", "dplyr"), silent = TRUE)
  try(conflict_prefer("select", "dplyr"), silent = TRUE)
}

```


```{r helper-functions, include=FALSE}

#' Format a data frame as a styled flextable
pretty_df <- function(df,
                      title    = NULL,
                      fontsize = 10,
                      padding  = 5,  # defined here
                      n        = 5) {
  if (!is.data.frame(df)) {
    stop("Input must be a data frame")
  }

  # Optional row slicing
  if (n > 0) df <- head(df, n)
  if (n < 0) df <- tail(df, abs(n))

  ft <- flextable::flextable(df) |>
    flextable::fontsize(size = fontsize, part = "all") |>
    flextable::padding(padding = padding, part = "all") |>
    flextable::align(align = "center", part = "all") |>
    flextable::theme_booktabs() |>
    flextable::bold(i = 1, part = "header") |>
    flextable::autofit()

  # Estimate and optionally adjust for long titles
  if (!is.null(title)) {
    estimated_char_width <- 0.07
    title_width <- nchar(title) * estimated_char_width

    if (!is.null(ft$body$colwidths)) {
      current_width <- sum(ft$body$colwidths, na.rm = TRUE)

      if (title_width > current_width && current_width > 0) {
        scale_factor <- title_width / current_width
        new_widths <- ft$body$colwidths * scale_factor

        for (j in seq_along(new_widths)) {
          ft <- flextable::width(ft, j = j, width = new_widths[j])
        }
      }
    }

    ft <- flextable::set_caption(ft, caption = title)
  }

  return(ft)
}


#' Format a confusion matrix as flextable
pretty_cm <- function(cm, title = "Confusion Matrix") {
  df <- as.data.frame.matrix(cm)
  pretty_df(df, title = title)
}


#' Format classification metrics
pretty_metrics <- function(truth, predicted, title = "Classification Metrics") {
  tab <- yardstick::metrics_vec(truth = truth, estimate = predicted, estimator = "macro")
  df <- as.data.frame(tab)
  pretty_df(df, title = title)
}


#' Summarise train/test splits
pretty_split <- function(train, test, target, title = "Train/Test Split Summary") {
  s <- function(data) {
    tbl <- table(data[[target]])
    out <- data.frame(
      Total = nrow(data),
      Class_1 = tbl[1],
      Class_2 = tbl[2],
      Prop_1 = round(tbl[1] / sum(tbl), 3),
      Prop_2 = round(tbl[2] / sum(tbl), 3)
    )
    rownames(out) <- NULL
    out
  }
  summary_df <- rbind(
    cbind(Set = "Train", s(train)),
    cbind(Set = "Test", s(test))
  )
  pretty_df(summary_df, title = title)
}


# ---- Pretty csv_read with Type Summary and Preview ------
pretty_read_csv <- \(path, n = 5, col_names = TRUE, show_col_types = FALSE) {
  df <- readr::read_csv(path, show_col_types = FALSE, col_names = col_names)
  df <- as.data.frame(df)
  ft <- pretty_df(df, glue::glue("CSV: {path}"))
  return(list(df = df, ft = ft))
}


# ---- Pretty Excel Reader with Type Summary and Preview ----
pretty_read_xlsx <- \(path, sheet = 1, col_names = TRUE, n = 0) {
  df <- readxl::read_excel(path, sheet = sheet, col_names = col_names)
  df <- as.data.frame(df)
  types <- purrr::map_chr(df, typeof)
  type_df <- data.frame(Column = names(df), Type = types, stringsAsFactors = FALSE)
  ft <- pretty_df(type_df, glue::glue("XLSX Column Types: {path}"))
  return(list(df = df, ft = ft))
}


# ---- Pretty ggplot metadata summary ----
pretty_ggplot <- function(plot, title = "ggplot Summary") {
  if (!inherits(plot, "gg")) stop("Input must be a ggplot object.")

  plot_data <- tryCatch(plot$data, error = function(e) NULL)
  geoms <- sapply(plot$layers, function(layer) class(layer$geom)[1])
  mappings <- plot$mapping

  global_aes  <- names(mappings)
  global_vals <- sapply(mappings, function(x) rlang::expr_text(x))

  p_title <- plot$labels$title %||% title %||% ""
  x_lab   <- plot$labels$x %||% ""
  y_lab   <- plot$labels$y %||% ""
  colour_lab <- plot$labels$colour %||% plot$labels$color %||% ""

  df <- data.frame(
    Component = c("Title", "X Axis", "Y Axis", "Colour Legend", "Geoms", global_aes),
    Value     = c(p_title, x_lab, y_lab, colour_lab, paste(geoms, collapse = ","), global_vals),
    stringsAsFactors = FALSE
  )
  pretty_df(df)
}


#' Pretty glm model summary
pretty_glm <- function(model, title = "GLM Model Summary" ) {
  if (!inherits(model, "glm")) {
    stop("pretty_glm() expects a glm object.")
  }
  tryCatch({
    df <- data.frame(
      Metric = c(
        "Formula", "AIC", "Null deviance", "Residual deviance",
        "Component names", "Model class"
      ),
      Value = c(
        deparse(formula(model)),
        AIC(model),
        model$null.deviance,
        model$deviance,
        paste(names(model), collapse = ", "),
        paste(class(model), collapse = ", ")
      ),
      stringsAsFactors = FALSE
    )
    ft <- pretty_df(df, title = "GLM Model Summary")
  }, error = function(e) {
    cat("Error in pretty_glm():", conditionMessage(e), "\n")
    NULL
  })
  return(ft)
}

pretty_multinom <- function(model, title = "Multinomial Model Summary") {
  if (!inherits(model, "multinom")) {
    stop("pretty_multinom() expects a multinom object.")
  }
#browser()
    #print(paste(deparse(formula(model)), collapse = ""))  
    #print(AIC(model))
    #print(model$converged)
  
    #Print(paste(class(model), collapse = ", ")) 
    #print(paste(names(model), collapse = ", "))
  tryCatch({
    # Summary table
    df_summary <- data.frame(
  Metric = c(
    "Formula", "AIC", "Converged", "Number of Iterations",
    "Model class", "Component names"
  ),
  Value = c(
    paste(deparse(formula(model)), collapse = ""),
    AIC(model),
    if (!is.null(model$converged)) model$converged else "N/A",
    if (!is.null(model$iters)) model$iters else "N/A",
    paste(class(model), collapse = ", "),
    paste(names(model), collapse = ", ")
  ),
  stringsAsFactors = FALSE
)


    ft_summary <- pretty_df(df_summary, title = title, n = nrow(df_summary))

    # Coefficients table
    coef_mat <- coef(model)
    coef_df <- as.data.frame(
      apply(coef_mat, 2, function(x) formatC(x, digits = 3, format = "e")),
      stringsAsFactors = FALSE
    )
    coef_df$Class <- rownames(coef_mat)
    coef_df <- coef_df[, c("Class", setdiff(names(coef_df), "Class"))]

    ft_coef <- pretty_df(coef_df, title = "Multinomial Model Coefficients")

    return(list(ft_summary, ft_coef))
  }, error = function(e) {
    cat("Error in pretty_multinom():", conditionMessage(e), "\n")
    NULL
  })
}



#' Pretty split df into multiple flextables
pretty_split_df <- function(df,
                            cols = 6,
                            title = NULL,
                            fontsize = 10,
                            n = 5) {
  if (!is.data.frame(df)) {
    stop(cat("\\textit{Object is not a data frame:}", deparse(df)))
  }

  title <- if (is.null(title)) deparse(substitute(df)) else title
  df_show <- if (n > 0) head(df, n) else if (n < 0) tail(df, abs(n)) else df
  col_groups <- split(names(df_show), ceiling(seq_along(df_show) / cols))

  ft_list <- lapply(seq_along(col_groups), function(i) {
    subdf <- df_show[, col_groups[[i]], drop = FALSE]
    pretty_df(
      subdf,
      title = paste0(title, " (", i, ")"),
      fontsize = fontsize,
      n = n
    )
  })

  names(ft_list) <- paste0(
    title,
    " (",
    seq_along(ft_list),
    ")"
  )

  return(ft_list)
}


#' Generate a full QDA summary with priors, centroids, and equations
pretty_qda <- function(qda_model) {
  response_var <- as.character(attr(qda_model$terms, "variables")[[2]])
  predictor_vars <- attr(qda_model$terms, "term.labels")

  qda_equation <- paste0(
    "P(", response_var, " = k | ", paste(predictor_vars, collapse = ", "),
    ") ∝ π_k × f_k(", paste(predictor_vars, collapse = ", "), ")"
  )

  centroids_df <- as.data.frame(qda_model$means)
  centroids_df[[response_var]] <- rownames(centroids_df)
  centroids_df <- centroids_df[, c(response_var, setdiff(names(centroids_df), response_var))]

  priors_df <- as.data.frame(qda_model$prior)
  priors_df[[response_var]] <- rownames(priors_df)
  colnames(priors_df) <- c("prior_probability", response_var)
  priors_df <- priors_df[, c(response_var, "prior_probability")]

  return(list(
    centroids = pretty_df(centroids_df,qda_equation),
    priors    = priors_df,pretty_df(priors_df,qda_equation),
    equation = qda_equation
  ))
}


#' Format classification thresholds summary
pretty_thresh <- function(predicted_probs, truth, threshold = 0.5, positive_class = NULL, title = "Threshold-Based Classification Summary") {
  predicted_class <- ifelse(predicted_probs >= threshold, positive_class, paste0("not_", positive_class))
  cm <- table(Truth = truth, Predicted = predicted_class)
  pretty_cm(cm, title = title)
}


#' Summarise variable transformations
pretty_transforms <- function(transform_map, title = "Variable Transformation Mapping") {
  df <- as.data.frame(transform_map)
  names(df) <- c("Original", "Transformed")
  pretty_df(df, title = title)
}


#' Summarise missing value patterns
pretty_missing <- function(df, title = "Missing Data Summary") {
  missings <- sapply(df, function(x) sum(is.na(x)))
  df_out <- data.frame(Variable = names(missings), Missing_Count = missings)
  df_out <- df_out[df_out$Missing_Count > 0, ]
  pretty_df(df_out, title = title)
}


#' Display model object summary (generic use for tree, lm, etc.)
pretty_model <- function(model, title = "Model Summary") {
  summary_text <- capture.output(summary(model))
  df <- data.frame(Summary = summary_text)
  pretty_df(df, title = title)
}


 


#' Pretty summary for numeric columns
pretty_summary <- function(df) {
  ft <- dplyr::select(df, where(is.numeric)) |>
    dplyr::summarise(
      dplyr::across(
        dplyr::everything(),
        .fns = list(
          Mean     = ~mean(.x, na.rm = TRUE),
          Median   = ~median(.x, na.rm = TRUE),
          Min      = ~min(.x, na.rm = TRUE),
          Max      = ~max(.x, na.rm = TRUE),
          IQR      = ~IQR(.x, na.rm = TRUE),
          nNA      = ~sum(is.na(.x))
        )
      )
    ) |>
    tidyr::pivot_longer(
      cols = dplyr::everything(),
      names_to = c("Variable", ".value"),
      names_sep = "_"
    ) |>
    dplyr::mutate(dplyr::across(where(is.numeric), round, 2)) |>
    flextable::flextable() |>
    flextable::set_header_labels(
      Variable = "Feature",
      Mean     = "Mean",
      Median   = "Median",
      Min      = "Min",
      Max      = "Max",
      IQR      = "Interquartile Range",
      nNA      = "Missing Values"
    ) |>
    flextable::autofit() |>
    flextable::theme_vanilla()

  ft
}

pretty_model_comparison <- function(model_list) {
  stopifnot(is.list(model_list))
  
  # Storage
  comparison <- data.frame()
  
  for (name in names(model_list)) {
    entry <- model_list[[name]]
    
    # Required elements in each entry
    actual <- entry$actual
    predicted <- entry$predicted
    
    # Generate confusion matrix
    cm <- make_conf_matrix(predicted, actual)
    metrics <- eval_conf_matrix(cm)
    
    comparison <- rbind(comparison, data.frame(
      Model        = name,
      Accuracy     = round(metrics[2], 3),
      Precision    = round(metrics[3], 3),
      Recall       = round(metrics[4], 3),
      F1_Score     = round(metrics[5], 3),
      MisclassRate = round(metrics[1], 3)
    ))
  }
  
  # Reorder columns to preference
  comparison <- comparison[, c("Model", "Accuracy", "Precision", "Recall", "F1_Score", "MisclassRate")]
  
  # Return as pretty flextable
  pretty_df(comparison, title = "Model Performance Comparison")
  
  
  
}



```



```{r wsb-global-model-utils}
# --- Model Evaluation Utilities ---

model_boolean_eval <- function(model, pred_class, actual_class, train_data, threshold = 0.5) {
  pvals <- broom::tidy(model)$p.value
  sig_all <- all(pvals < 0.05)

  test_acc <- mean(pred_class == actual_class)
  base_acc <- max(prop.table(table(actual_class)))
  better_than_random <- test_acc > 0.5

  better_than_baseline_mild     <- (test_acc - base_acc) >= 0.01
  better_than_baseline_moderate <- (test_acc - base_acc) >= 0.015
  better_than_baseline_strong   <- (test_acc - base_acc) >= 0.02
  real_world_gain               <- (test_acc - base_acc) >= 0.05

  train_probs <- predict(model, newdata = train_data, type = "response")
  train_class <- ifelse(train_probs > threshold, 1, 0)
  train_acc <- mean(train_class == train_data$DEATH)
  well_generalised <- abs(train_acc - test_acc) < 0.05

  list(
    sig_all = sig_all,
    test_acc = test_acc,
    base_acc = base_acc,
    better_than_random = better_than_random,
    better_than_mild = better_than_baseline_mild,
    better_than_moderate = better_than_baseline_moderate,
    better_than_strong = better_than_baseline_strong,
    well_generalised = well_generalised,
    real_world_gain = real_world_gain
  )
}

summarise_boolean_eval <- function(eval_list) {
  data.frame(
    Question = c(
      "Statistically significant?",
      "Better than random?",
      "Better than majority guess? (≥ 0.01)",
      "Better than majority guess? (≥ 0.015)",
      "Better than majority guess? (≥ 0.02)",
      "Well-generalised?",
      "Real-world useful?"
    ),
    Result = unlist(c(
      eval_list$sig_all,
      eval_list$better_than_random,
      eval_list$better_than_mild,
      eval_list$better_than_moderate,
      eval_list$better_than_strong,
      eval_list$well_generalised,
      eval_list$real_world_gain
    )),
    Justification = c(
      "all(pvals < 0.05)",
      "test_acc > 0.5",
      "(test_acc - base_acc) >= 0.01",
      "(test_acc - base_acc) >= 0.015",
      "(test_acc - base_acc) >= 0.02",
      "abs(train_acc - test_acc) < 0.05",
      "(test_acc - base_acc) >= 0.05"
    ),
    stringsAsFactors = FALSE
  )
}


#Convert probabilities to labelled factor predictions
predict_classes <- function(probs, threshold = 0.5) {
  factor(ifelse(probs > threshold, 1, 0), levels = c(0, 1), labels = c("LIVE", "DEAD"))
}

# 2. Create a labelled confusion matrix from predicted and actual class labels
make_conf_matrix <- function(pred_class, actual_class) {
  table(
    Predicted = factor(pred_class, levels = c("LIVE", "DEAD")),
    Actual    = factor(actual_class, levels = c("LIVE", "DEAD"))
  )
}

# 3. Compute performance metrics from a 2x2 labelled confusion matrix
# Returns: misclassification, accuracy, precision, recall, F1
eval_conf_matrix <- function(cmtx) {
  tp <- cmtx["DEAD", "DEAD"]
  tn <- cmtx["LIVE", "LIVE"]
  fp <- cmtx["DEAD", "LIVE"]
  fn <- cmtx["LIVE", "DEAD"]
  total <- sum(cmtx)

  if (total == 0 || any(is.na(c(tp, tn, fp, fn)))) return(rep(NA, 5))

  accuracy  <- (tp + tn) / total
  precision <- if ((tp + fp) > 0) tp / (tp + fp) else NA
  recall    <- if ((tp + fn) > 0) tp / (tp + fn) else NA
  f1        <- if (!is.na(precision) && !is.na(recall) && (precision + recall) > 0)
                 2 * precision * recall / (precision + recall) else NA
  misclass  <- 1 - accuracy

  c(misclass, accuracy, precision, recall, f1)
}

# 4. Convert a vector of named metrics into a summary data frame
summarise_metrics <- function(metrics_vec) {
  metric_names <- c("Misclassification Rate", "Accuracy", "Precision (DEAD)", "Recall (DEAD)", "F1 Score (DEAD)")
  data.frame(Metric = metric_names, Value = round(metrics_vec, 3), stringsAsFactors = FALSE)
}
```



```{r load-and-test-split, results='hide'}
# Load heart and colour datasets from zip

# Set base directory
# Assign named paths based on filename match
# load the dataframes via pretty_read_csv 
# preview structure
unzip_dir <- "../data/unzipped"
zip_path  <- "../data/data_assignment_2.zip"
csv_files <- unzip(zip_path, list = TRUE)$Name  # Extract filenames from the zip
target_paths <- file.path(unzip_dir, csv_files)

idx_color   <- grep("color", csv_files)
idx_heart   <- grep("heart", csv_files)
path_colour <- target_paths[idx_color]
path_heart  <- target_paths[idx_heart]
 
heart_list  <- pretty_read_csv(path_heart, col_names = TRUE)
color_ft <- pretty_read_csv(path_colour, col_names = TRUE)
   
heart_df = heart_list$df
heart_ft = heart_list$ft 
                                                         
color_df = color_ft$df
color_ft = color_ft$ft 


foo <- pretty_split_df(heart_df)

render_flextables <- function(ft_list) {
  for (ft in foo) {
    invisible(print(knitr::knit_print(ft)))
    }
  }

render_flextables(foo)
```
 

```{r wsb-step-03-split}
cat("### Split the dataset into a training set (80%) and a test set (20%)
")

# heart_df exists at run time

h_vars <- c("DEATH", "GLUCOSE", "SYSBP", "AGE")
h_clean <- heart_df |>
  select(all_of(h_vars)) |>
  drop_na() |>
  mutate(DEATH = as.factor(DEATH))

# Stratified split : DEATH
set.seed(82171165)
h_split   <- initial_split(h_clean, prop = 0.8, strata = DEATH)
h_train   <- training(h_split)
h_test    <- testing(h_split)

# For visualisation only: relabel  "LIVE" and "DEAD"
levels = c(0, 1)
labels = c("LIVE", "DEAD")


hp_clean <- h_clean |>
  mutate(DEATH = factor(DEATH, levels = levels, labels = labels))
hp_train <- h_train |>
  mutate(DEATH = factor(DEATH, levels = levels, labels = labels))
hp_test <- h_test |>
  mutate(DEATH = factor(DEATH, levels = levels, labels = labels))

h_actual_class  <- h_test$DEATH        # true observed values
hp_actual_class <- hp_test$DEATH

```
### Visualise the relationship between DEATH, GLUCOSE and SYSBP
 
```{r wsb-step-04-visual}
cat("### Visualise the relationship between DEATH, GLUCOSE and SYSBP\n")

ggp_relationship <- ggplot(hp_clean, aes(x = GLUCOSE, y = SYSBP, colour = DEATH)) +
  geom_point(alpha = 0.6) +
  labs(title = "DEATH vs GLUCOSE and SYSBP", x = "GLUCOSE", y = "SYSBP")

# 1. Distribution of SYSBP
ggp_sysbp <- ggplot(hp_test, aes(x = SYSBP)) +
  geom_histogram(bins = 40, fill = "steelblue", colour = "white") +
  labs(title = "Distribution of SYSBP", x = "SYSBP", y = "Count")

# 2. Distribution of GLUCOSE
ggp_glucose <- ggplot(hp_test, aes(x = GLUCOSE)) +
  geom_histogram(bins = 40, fill = "darkorange", colour = "white") +
  labs(title = "Distribution of GLUCOSE", x = "GLUCOSE", y = "Count")

# 3–6. Combination of linear/log axis scales
base <- ggplot(hp_test, aes(x = GLUCOSE, y = SYSBP)) +
  geom_point(alpha = 0.6) +
  labs(title = NULL, x = "GLUCOSE", y = "SYSBP")

p_lin_lin <- base + ggtitle("Linear X / Linear Y")
p_log_lin <- base + scale_x_log10() + ggtitle("Log X / Linear Y")
p_lin_log <- base + scale_y_log10() + ggtitle("Linear X / Log Y")
p_log_log <- base + scale_x_log10() + scale_y_log10() + ggtitle("Log X / Log Y")

# Combine 4 into a grid
p_combined <- (p_lin_lin | p_log_lin) / (p_lin_log | p_log_log)

# Output all plots
print(ggp_relationship)
print(ggp_sysbp)
print(ggp_glucose)
print(p_combined)

pretty_ggplot(ggp_relationship, title = "Relationship: DEATH vs GLUCOSE and SYSBP")
pretty_ggplot(ggp_sysbp, title = "Distribution of SYSBP")
pretty_ggplot(ggp_glucose, title = "Distribution of GLUCOSE")
pretty_ggplot(p_lin_lin, title = "Linear X / Linear Y")
pretty_ggplot(p_log_lin, title = "Log X / Linear Y")
pretty_ggplot(p_lin_log, title = "Linear X / Log Y")
pretty_ggplot(p_log_log, title = "Log X / Log Y")
pretty_ggplot(p_combined, title = "Log/Linear Scale Comparison Grid")

```

### Form an initial hypothesis of what to look for when doing the classification

 ->
```{r  heart-wsb-step-03}
print("We hypothesise that high GLUCOSE and high SYSBP are associated with higher DEATH risk.")
```

### Fit a logistic regression model on the training set

 
```{r  heart-wsb-step-04}
               
equation_lg <- DEATH ~ GLUCOSE + SYSBP + AGE
hp_model <- glm(equation_lg, data = hp_train, family = "binomial")
pretty_glm(hp_model)


```


```{r wsb-step-05-model}
# Fit a logistic regression model on training data
# Summarize using pretty_glm
 
pretty_glm(hp_model)
```





### Compute the misclassification rate on the test set

```{r wsb-step-06}
cat("### Compute the misclassification rate on the test set\n")

# Generate predicted probabilities and classes
hp_pred_probs <- predict(hp_model, newdata = hp_test, type = "response")
hp_pred_class <- predict_classes(hp_pred_probs, threshold = 0.5)

# Compute actual class labels
hp_actual_class <- factor(hp_test$DEATH, levels = c("LIVE", "DEAD"))

# Calculate confusion matrix
hp_cmtx <- make_conf_matrix(hp_pred_class, hp_actual_class)

# Evaluate confusion matrix
hp_metrics <- eval_conf_matrix(hp_cmtx)
hp_mtrx_df <- summarise_metrics(hp_metrics)

# Convert confusion matrix to printable data frame
hp_cmtx_df <- as.data.frame.matrix(hp_cmtx)
hp_cmtx_df <- cbind(Predicted = rownames(hp_cmtx_df), hp_cmtx_df)
rownames(hp_cmtx_df) <- NULL

# Print formatted summaries
print(pretty_df(hp_cmtx_df, title = "Logistic Regression Confusion Matrix (Threshold = 0.5)"))
print(pretty_df(hp_mtrx_df, title = "Logistic Regression Performance (Threshold = 0.5)"))
```
 
  
### Comment on the goodness of fit
```{r  heart-wsb-step-08}
#refere to hp_mtrx_df 
```

### Supporting numeric metrics for evaluation criteria

 
```{r  heart-wsb-step-09}
hp_pred_class_10 <- ifelse(hp_pred_probs > 0.1, 1, 0)
hp_pred_class_10 <- factor(hp_pred_class_10, levels = c(0, 1), labels = c("LIVE", "DEAD"))
```

### Recompute misclassification, confusion, and visualisation for 10% threshold
# Confusion Matrix (Threshold = 0.10)


 
```{r wsb-step-10}
#hp_misclass_rate_10 <- mean(hp_pred_class_10 != hp_actual_class)
#hp_cmtx_10 = table(Predicted = h_pred_class_10, Actual = h_actual_class)
```

### Compare logistic regression and discriminant analysis
```{r heart-wsb-step-10.1}
cat("### Logistic Regression Evaluation (Threshold = 0.10)\n")

hp_pred_probs <- predict(hp_model, newdata = hp_test, type = "response")
hp_pred_class_10 <- predict_classes(hp_pred_probs, threshold = 0.10)
hp_actual_class <- factor(hp_test$DEATH, levels = c("LIVE", "DEAD"))

hp_cmtx_test_10 <- make_conf_matrix(hp_pred_class_10, hp_actual_class)
hp_metrics_10 <- eval_conf_matrix(hp_cmtx_test_10)
hp_mtrx_df_10 <- summarise_metrics(hp_metrics_10)

hp_cmtx_df_10 <- as.data.frame.matrix(hp_cmtx_test_10)
hp_cmtx_df_10 <- cbind(Predicted = rownames(hp_cmtx_df_10), hp_cmtx_df_10)
rownames(hp_cmtx_df_10) <- NULL

print(pretty_df(hp_cmtx_df_10, title = "Confusion Matrix (Threshold = 0.10)"))
print(pretty_df(hp_mtrx_df_10, title = "Logistic Regression Performance (Threshold = 0.10)"))
```

### QDA Model
 
```{r heart-wsb-step-11}
hp_qda_model <- qda(equation_lg, data = hp_train) 
```


 
### QDA Evaluation
  a
```{r heart-wsb-step-11.1}
cat("### QDA Evaluation\n")

# Build QDA model using labeled data
hp_qda_model <- qda(equation_lg, data = hp_train)

# Predict posterior probabilities and classes
qda_output <- predict(hp_qda_model, newdata = hp_test)
hp_qda_pred_probs <- qda_output$posterior[, "DEAD"]
hp_qda_pred_class <- predict_classes(hp_qda_pred_probs, threshold = 0.5)

# Define actual classes
hp_actual_class <- factor(hp_test$DEATH, levels = c("LIVE", "DEAD"))

# Confusion matrix and metrics
hp_cmtx_qda <- make_conf_matrix(hp_qda_pred_class, hp_actual_class)
hp_metrics_qda <- eval_conf_matrix(hp_cmtx_qda)
hp_mtrx_df_qda <- summarise_metrics(hp_metrics_qda)

# Format confusion matrix for display
hp_cmtx_df_qda <- as.data.frame.matrix(hp_cmtx_qda)
hp_cmtx_df_qda <- cbind(Predicted = rownames(hp_cmtx_df_qda), hp_cmtx_df_qda)
rownames(hp_cmtx_df_qda) <- NULL

# Print results
print(pretty_df(hp_cmtx_df_qda, title = "QDA Confusion Matrix"))
print(pretty_df(hp_mtrx_df_qda, title = "QDA Model Performance"))

```
 

### Visualise the decision boundaries of the fitted QDA model in a suitable way
 
```{r heart-wsb-step-12}
print("Visualise the decision boundaries of the fitted QDA model in a suitable way.")

# A simple visualisation by predicted class
hp_qda_probs <- predict(hp_qda_model, newdata = hp_test)$posterior[, 2]
hp_qda_pred_class <- predict_classes(hp_qda_probs, threshold = 0.5)


ggp_qda_boundary <- ggplot(hp_test, aes(x = GLUCOSE, y = SYSBP, colour = hp_qda_pred_class)) +
  geom_point(alpha = 0.5) +
  labs(title = "QDA Decision Boundary (approx)", colour = "Predicted")



print(ggp_qda_boundary)
```

### Identify strong risk factors and communicate results


 
### Fit a tree-based model to the training set
 
```{r heart-wsb-step-13}
cat("### Fit a Tree-Based Model to the Training Set\n")

# Fit classification tree to hpl_train
hp_tree_model <- tree(equation_lg, data = hp_train)

# Optional: Display summary of the model
print(summary(hp_tree_model))

# Optional: Visualise the tree
plot(hp_tree_model)
text(hp_tree_model, pretty = 0)

```

### Tree-Based or Optional Model Evaluation
```{r heart-wsb-step-13.1}
cat("### Tree-Based or Optional Model Evaluation\n")

# Generate predicted probabilities and classes
hp_pred_probs <- predict(hp_tree_model, newdata = hp_test, type = "vector")
hp_pred_class <- predict_classes(hp_pred_probs[, "DEAD"], threshold = 0.5)
hp_actual_class <- factor(hp_test$DEATH, levels = c("LIVE", "DEAD"))

# Build confusion matrix
hp_cmtx <- make_conf_matrix(hp_pred_class, hp_actual_class)
hp_metrics <- eval_conf_matrix(hp_cmtx)
hp_mtrx_df <- summarise_metrics(hp_metrics)

# Format for display
hp_cmtx_df <- as.data.frame.matrix(hp_cmtx)
hp_cmtx_df <- cbind(Predicted = rownames(hp_cmtx_df), hp_cmtx_df)
rownames(hp_cmtx_df) <- NULL

# Print both confusion matrix and metrics
print(pretty_df(hp_cmtx_df, title = "Tree-Based Model Confusion Matrix"))
print(pretty_df(hp_mtrx_df, title = "Tree-Based Model Performance"))

```

```{r heart-wsb-step-14}

hp_pred_probs <- predict(hp_model, newdata = hp_test, type = "response")
hp_pred_glm   <- predict_classes(hp_pred_probs, threshold = 0.5)

qda_output   <- predict(hp_qda_model, newdata = hp_test)
hp_pred_qda  <- predict_classes(qda_output$posterior[, "DEAD"], threshold = 0.5)


hp_pred_probs_tree <- predict(hp_tree_model, newdata = hp_test, type = "vector")
hp_pred_tree <- predict_classes(hp_pred_probs_tree[, "DEAD"], threshold = 0.5)

pretty_glm(hp_model)
pretty_qda(hp_qda_model)$equation
pretty_model_comparison(list(
  GLM  = list(actual = hp_actual_class, predicted = hp_pred_glm),
  QDA  = list(actual = hp_actual_class, predicted = hp_pred_qda),
  TREE = list(actual = hp_actual_class, predicted = hp_pred_tree)
))




```



### Step 1: Load and preview the Colour dataset
```{r colour-wsb-step-01-load} 
# Load heart and colour datasets from zip

print(color_ft)
```
 


### Step 2: Clean and prepare data
```{r colour-wsb-step-02-clean}
c_vars <- c("color", "r", "b")
c_clean <- color_df |>
  select(all_of(c_vars)) |>
  drop_na()

c_clean <- c_clean |>
  mutate(color = factor(color))
```


### Step 3: Train/test split
```{r colour-wsb-step-03-split}
set.seed(123)
c_split <- initial_split(c_clean, prop = 0.8, strata = color)
c_train <- training(c_split)
c_test  <- testing(c_split)

#c_train <- c_train |>
#  mutate(colour = factor(color))
#cp_test  <- c_test |>
#  mutate(colour = factor(color))
#cp_actual_class <- cp_test$color
```


### Step 4: Visualise class distribution and feature relationships
```{r colour-wsb-step-04-visualise}
c_plot_dist <- ggplot(c_clean, aes(x = r, y = b, color = color)) +
  geom_point(alpha = 0.6) + labs(title = "color Classes by R and B")
 
c_plot_train <- ggplot(c_train, aes(x = r, y = b, color = color)) +
  geom_point(alpha = 0.6) + labs(title = "color train")
c_plot_test <- ggplot(c_test, aes(x = r, y = b, color = color)) +
  geom_point(alpha = 0.6) + labs(title = "color test")
print(c_plot_dist)
 
print(c_plot_train)
print(c_plot_test)
print(pretty_ggplot(c_plot_dist, title = "Class Distribution: r vs b"))
 
print(pretty_ggplot(c_plot_train, title = "Class train: r vs b"))
print(pretty_ggplot(c_plot_test, title = "Class test: r vs b"))
```


### Step 5: Fit logistic regression model
```{r colour-wsb-step-05-model}
c_equation_lg = color ~ r + b
c_model <- multinom(c_equation_lg, data = c_train, family = "multinomial")
list_of_df <- pretty_multinom(c_model)

if (!is.null(list_of_df)) {
  for (ft in list_of_df) {
    print(ft)
  }
}



```


### Step 6: Evaluate logistic model on test set
```{r colour-wsb-step-06-eval}

c_actual_class <- c_test$color

c_pred_glm <- predict(c_model, newdata = c_test, type = "class")
c_cmtx_glm <- make_conf_matrix(c_pred_glm, c_actual_class)
c_metrics_glm <- eval_conf_matrix(c_cmtx_glm)
c_mtrx_df_glm <- summarise_metrics(c_metrics_glm)

print(pretty_cm(c_cmtx_glm, title = "GLM Confusion Matrix"))
print(pretty_df(c_mtrx_df_glm, title = "GLM Performance Metrics"))
```


### Step 7: Fit QDA model
```{r colour-wsb-step-07-qda-model}
c_qda_model <- qda(c_equation_lg, data = c_train)
qda_info <- pretty_qda(c_qda_model)
print(qda_info)
 
#print(pretty_df(qda_info$centroids, title = "QDA Class Centroids"))
#print(pretty_df(qda_info$priors, title = "QDA Prior Probabilities"))
cat("Symbolic QDA Equation:\n", qda_info$equation)
```


### Step 8: Evaluate QDA model
```{r colour-wsb-step-08-qda-eval}
c_qda_probs <- predict(c_qda_model, newdata = c_test)$posterior
c_pred_qda  <- colnames(c_qda_probs)[apply(c_qda_probs, 1, which.max)]
c_pred_qda  <- factor(c_pred_qda, levels = levels(c_actual_class))

c_cmtx_qda <- make_conf_matrix(c_pred_qda, c_actual_class)
c_metrics_qda <- eval_conf_matrix(c_cmtx_qda)
c_mtrx_df_qda <- summarise_metrics(c_metrics_qda)

print(pretty_cm(c_cmtx_qda, title = "QDA Confusion Matrix"))
print(pretty_df(c_mtrx_df_qda, title = "QDA Performance Metrics"))
```


### Step 9: Predict Colour(200, 0, 200)
```{r colour-wsb-step-09-predictRGB}
new_colour <- data.frame(r = 200, b = 0)

# GLM prediction
glm_class <- predict(c_model, newdata = new_colour, type = "class")

# QDA prediction
qda_probs <- predict(c_qda_model, newdata = new_colour)$posterior
qda_class <- colnames(qda_probs)[which.max(qda_probs)]

cat("Prediction for color(200, 0, 200):\n")
cat("GLM: ", glm_class, "\n")
cat("QDA: ", qda_class, "\n")
```


### Step 10: Compare model performance
```{r colour-wsb-step-10-compare}
print(pretty_model_comparison(list(
  GLM  = list(actual = c_actual_class, predicted = c_pred_glm),
  QDA  = list(actual = c_actual_class, predicted = c_pred_qda)
)))
```


### Step 11: Summary and discussion
```{r colour-wsb-step-11-summary}
cat("\nModel Summary Notes:\n")
cat("- Logistic regression worked well for separating linear boundaries.\n")
cat("- QDA captured more complex class regions based on priors and covariances.\n")
cat("- Predictions for Colour(200, 0, 200) showed some model disagreement.\n")



```



```


c_train <- 1
## Introduction

This report presents a comprehensive statistical modeling analysis conducted for Assignment 2 of STAT462. The objective is to develop and evaluate predictive models for a multi-class classification problem using color data. The response variable of interest is `colour`, which consists of five categories: red, blue, pink, purple, and brown. The predictor variables are RGB color values (`r` and `b`), extracted from images.

Two modeling approaches are explored and compared:

1. **Quadratic Discriminant Analysis (QDA)** — a generative classification model that fits a separate multivariate normal distribution for each class and allows each class to have its own covariance matrix.

2. **k-Nearest Neighbors (k-NN)** — a non-parametric method that classifies a point based on the majority class among its k nearest neighbors in the feature space.

Throughout the report, these models are evaluated based on their classification accuracy, decision boundaries, and ROC curves (using one-vs-rest strategy) to assess their ability to distinguish each class from the others.

The analysis proceeds by first introducing the dataset, followed by model development, performance evaluation, and interpretation of results.


# 2. Data

The dataset used in this analysis contains RGB color values (`r`, `g`, `b`) extracted from image samples, with an associated color label stored in the `colour` variable. The objective is to classify these color labels based on the numeric RGB values.

After loading, the dataset was split into training and testing sets. The training set is used to build the models, while the testing set is used for performance evaluation.
```{r}

```

```{r data-class-distribution, echo=FALSE, message=FALSE, warning=FALSE}
# Generate class summary from the training data
#c_class_summary <- c_train %>%
 # count(colour, name = "Count") %>%
#  mutate(Proportion = round(Count / sum(Count), 3))

# Display formatted table
#pretty_df(c_class_summary, title = "Class Distribution in Training Set")
```

This table (Figure \@ref(fig:class-distribution)) shows the number and proportion of observations for each color class in the training set.
le above shows how many samples belong to each color class and what proportion they represent in the training data. This helps assess class balance and potential modeling challenges.


# Describe the dataset, variables, and preparation
```

# 3. Methodology

```{r methodology}
# Describe QDA and k-NN, tuning, training approach
```

# 4. Model Evaluation

```{r evaluation}
# Accuracy, ROC curves, confusion matrices
```

# 5. Model Comparison

```{r comparison}
# Compare QDA vs kNN using metrics and visuals
```

# 6. Discussion

```{r discussion}
# Interpret the findings, implications
```

# 7. Conclusion

```{r conclusion}
# Summarize the report, restate findings, recommendations
```

# References

```{r references, include=FALSE}
# BibTeX or manual citation list
```

# Appendix

```{r appendix}
# Additional plots or tables
```
