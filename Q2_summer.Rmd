---
title: "Question B: Predicting color name from RGB values, using discriminant analysis"
author: "Summer"
date: "`r Sys.Date()`"
output: html_document
---

```{r Environment Set up, include=F,eval=T}
# DEE: This chunk was set up with the aid of ChatGPT
#      The intent is to load updates quietly thus not
#      spending undue time with the logistics of getting 
#      setup. 

options(repos = c(CRAN = "https://cran.stat.auckland.ac.nz/"))

# Required packages
#
required_packages <- c("conflicted", "ggplot2", "dplyr"
                       
                       ) 

# Install and load missing packages in a single step and quietly
for (pkg in required_packages) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
  }
  library(pkg, character.only = TRUE)  # Load package after installation check
}

conflict_prefer("filter", "dplyr"); conflict_prefer("select", "dplyr")
conflicts_prefer(tidyr::expand);conflicts_prefer(plotly::layout);conflicts_prefer(pROC::auc)


knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, width = 70, cache=FALSE)
```

# Common Function Set Up

We will reuse some quite common functions from `Common Functions.R` to avoid redundancy, but we do implement QDA by hand.

```{r, include=FALSE}
# Function to split data into training and test sets
split_data <- function(data, train_ratio = 0.8) {
  # Set a seed for reproducibility and to minimize RAM usage
  set.seed(62380486) 
  # validate train_ratio range
  if (train_ratio <= 0 || train_ratio >= 1) {
    stop("Error: train_ratio must be between 0 and 1 (exclusive).")
  }
  # Randomly select the specified percentage of indices for the training set
  train_ind <- sample(1:nrow(data), 
                      size = floor(train_ratio * nrow(data)),
                      replace = FALSE)
  
  # Use the remaining indices for the test set
  test_ind <- setdiff(1:nrow(data), train_ind)
  
  # Create training data using the selected indices
  train_data <- data[train_ind, , drop = FALSE]
  rownames(train_data) <- NULL
  
  # Create test data using the remaining indices
  test_data <- data[test_ind, , drop = FALSE]
  rownames(test_data) <- NULL
  
  # Return both training and test data as a list
  return(list(train = train_data, test = test_data))
}
```

***In this question, you are not allowed to use*** ***any pre-implemented modules for performing discriminant analysis, but you’ll have to implement this yourself.***

# Background Information

Colors can be coded via their RGB (red, green, blue) value in the form `(r,g,b)`, where `r`, `g`, and `b` are integers between 00 and 255. For example, `(255,0,0)` is pure red, and `(128,200,128)` is a shade of green.

In this exercise we will map `(r,g,b)` values to their color names. For example, we want `(255,0,0)` to be classified as red.

In order to make things a bit easier, we focus on the part of color space where `g=0`, i.e. there is no green component. This means the feature space is all combinations `(r,0,b)`, where `r` and `b` are between 0 and 255. For orientation, here is a visualisation of some of these colors, with each circle having the color of its `r`-`b`-coordinate.

```{r}
rs <- seq(0,256,5) 
bs <- seq(0,256,5)
df_plot_colors <- data.frame(rs = rs, bs=bs) %>% tidyr::expand(rs, bs)
ggplot(data=df_plot_colors) + 
  geom_point(aes(x=rs, y=bs, color=rgb(rs/256,0,bs/256)), size=1)+
  # R's rgb code works with numbers between 0 and 1 instead of between 0 and 255.
  scale_color_identity() +
  theme(legend.position = "none")
```

Our goal is to create a classification algorithm that takes an `r` value, and a `b` value, and outputs the name of the color this corresponds to.

Thankfully, we have a dataset where some of these labels have been entered. This is visualised below

```{r}
df_colors <- read.csv("./data/unzipped/colors_train.csv")

df_colors_augmented <- df_colors %>% mutate(rgb = rgb(r/256,0,b/256))

ggplot(data=df_colors_augmented, aes( r, b, label = color)) +
  geom_point(aes(x=r, y=b, color=rgb), size=3) +
  geom_text(data = df_colors, check_overlap = TRUE) +
  scale_color_identity() +
  theme(legend.position = "none")
```

# Solution

## 1. How many classes are there in the dataset?

```{r}
classes <- df_colors_augmented %>% pull("color") %>% unique 
class_num <- classes %>% length
class_num
```

Therefore, there are 5 classes, which correspond to the following colors:

```{r}
classes
```

## 2. Fit a QDA algorithm to this classification problem and visualise the decision boundaries in a suitable way.

The general discriminant score function takes the following form:

$$ \delta_k(\underline x) = -\frac{(\underline x - \underline m_k)^\top C_k^{-1} (\underline x - \underline m_k)}{2} - \frac{\log \mathrm{det}(C_k)}{2} + \log \pi_k$$

Now we are using 2 features, `red:=df_colors_augmented$r` and `blue:=df_colors_augmented$b`, to predict the class: `color:= df_colors_augmented$color`.

For each class $X = \underline{x}|class=k$, we need to know its *mean vector(* $m_k$ *)*, *covariance matrix(* $C_{k}$ *)* and the prior probability( $\pi_k$*)* . Here $\underline{x}$ represents a vector corresponding to class $k$.

### Dataset Split

Use `split_data` function to split the dataset into a training set and a test set with the ratio of 8:2.

```{r}
# source("CommonFunctions.R")
df_colors <- split_data(data = df_colors_augmented,train_ratio = 0.8)
```

### Build up discriminant score function

We have 5 classes as below:

```{r}
df_colors$train %>% pull("color") %>% unique
```

To improve the efficiency of calculation process, we define a function `stat_cal` to solve $m_k, C_k, \pi_k$.

```{r}
# Function name: calculate_category_stats
# Arguments:
#   category_name: Category name (string)
#   df: DataFrame (pandas DataFrame)
# Returns:
#   A list containing the number of rows, column means, and covariance matrix for the category

stat_cal <- function(category_name, df) {
  # Filter the DataFrame for the specified category
  df_filtered <- df %>% filter(color == category_name)

  # Calculate the number of rows in the category
  n <- nrow(df_filtered)

  # Calculate the portion of rows in the category
  pi <- n/nrow(df)
  
  # Calculate the column means for 'r' and 'b'
  m <- df_filtered %>% dplyr::select(r, b) %>% colMeans
  
  # Calculate the covariance matrix for 'r' and 'b'
  Sigma <- df_filtered %>% dplyr::select(r, b) %>% cov
  
  return(list(pi = pi, m = m, Sigma = Sigma))
}

```

For all 5 classes, we can apply a `sapply` function to above calculation process to obtain the `stat_values` matrix, which combines $n, \pi, m, \sigma$ for each class.

```{r}
color_names=c("purple","pink","red","brown","blue")
color_stat_values <- sapply(color_names, function(color) {
  stat_cal(color, df = df_colors$train)
})

color_stat_values
```

N.B. We can verify the statistic values above by summing the `pi` values; the sum should equal 1. `pi` represents the prior probability, indicated by each class's frequency over all events.

```{r}
sum(unlist(color_stat_values["pi",]))
```

Then we define a function `delta` to calculate the delta value, given class input ( $X=x$ ) and its mean *vector*( $m_k$ ), prior probability( $\pi_k$ ) and *covariance* matrix( $C_k$ ).

```{r}
# Function: Calculate the discriminant function value (delta) for a given sample.
#
# Args:
#   X: A numeric vector representing an observation.
#   mean: The mean vector of the class.
#   Sigma: The covariance matrix of the class.
#   pi: The prior probability of the class.
#
# Returns:
#   The discriminant function value for the sample belonging to the class.

# Corrected delta function
delta <- function(X, mean, Sigma, pi){
  X_minus_mean <- X - mean
  Sigma_inv <- solve(Sigma)
  result <- -t(X_minus_mean) %*% Sigma_inv %*% X_minus_mean / 2 - log(det(Sigma))/2 + log(pi)
  return(result)
}
```

Now, we can apply function `delta` to calculate classes' delta values of a given class.

```{r}
X_purple <- df_colors$train %>% filter(color == "purple") %>% select(r, b)
X_purple_stat <- stat_cal("purple",df = df_colors$train)
delta_purple <- apply(X_purple, 1, function(row) {
  delta(X = as.numeric(row), # Convert row to numeric vector
        mean = X_purple_stat$m,
        Sigma = X_purple_stat$Sigma,
        pi = X_purple_stat$pi)
})
```

Similarly, we apply a loop function to above delta calculation process.

```{r}
# Create an empty list to store the results
delta_values <- list()

# Loop through each color category
for (color in color_names) {
  # 1. Extract data for the specific color
  X_color <- df_colors$train %>% 
    filter(color == !!color) %>%  # Use !! for variable substitution
    select(r, b)
  
  # 2. Calculate statistics
  X_color_stat <- stat_cal(color, df = df_colors$train)
  
  # 3. Calculate discriminant values
  delta_color <- apply(X_color, 1, function(row) {
    delta(X = as.numeric(row), # Convert row to numeric vector
          mean = X_color_stat$m,
          Sigma = X_color_stat$Sigma,
          pi = X_color_stat$pi)
  })
  
  # 4. Store the results in the list
  delta_values[[color]] <- list(
    X = X_color,
    stat = X_color_stat,
    delta = delta_color
  )
```

### Visualise The Decision Boundaries

Now we have stored all classes' delta values in list `delta_values` and we can call them by using `delta_values[["purple"]]$delta`. Then, we use these 5 delta values to visualise the decision boundary.

```{r}
# Generate a grid of points for contour or tile plot

plot_grid <- expand.grid(
  r = seq(min(df_colors$train$r), max(df_colors$train$r), length.out = 100),
  b = seq(min(df_colors$train$b), max(df_colors$train$b), length.out = 100)
)

# class those points with the discriminant functions 
plot_grid <- plot_grid %>% 
  rowwise() %>% 
  mutate(pred_by_hand = {
    purple_delta <- delta_values$purple$delta
    pink_delta <- delta_values$pink$delta
    red_delta <- delta_values$red$delta
    brown_delta <- delta_values$brown$delta
    blue_delta <- delta_values$blue$delta
    as.factor(max(purple_delta, pink_delta, red_delta, brown_delta, blue_delta))
  })

```

```{r}
delta_values$purple$delta

```

Test your algorithm on `(200,0,200)`. What color is this being called by your algorithm?

1.  *Opportunity for showing extra effort*: Learn about kNN-*Classification*, implement it, and compare it with your QDA algorithm’s results.

How many classes are there in the dataset? (数据集中有多少个类？)

-   Fit a QDA algorithm to this classification problem and visualise the decision boundaries in a suitable way. (将 QDA 算法拟合到这个分类问题，并以合适的方式可视化决策边界。)
-   Test your algorithm on (200,0,200). (在 (200,0,200) 上测试你的算法。)
-   What color is this being called by your algorithm? (你的算法将此颜色称为什么？)
-   Opportunity for showing extra effort: Learn about kNN-Classification, implement it, and compare it with your QDA algorithm’s results. (展示额外努力的机会：了解 kNN 分类，实现它，并将其与你的 QDA 算法的结果进行比较。)

**步骤 (Steps):**

1.  **确定类别数量 (Determine Number of Classes):**
    -   English: Determine the number of distinct color classes present in the dataset.
    -   中文: 确定数据集中存在的不同颜色类别的数量。
2.  **拟合 QDA 模型 (Fit QDA Model):**
    -   English: Fit a Quadratic Discriminant Analysis (QDA) algorithm to the classification problem.
    -   中文: 将二次判别分析 (QDA) 算法拟合到分类问题上。
3.  **可视化决策边界 (Visualize Decision Boundaries):**
    -   English: Visualize the decision boundaries of the fitted QDA model in a suitable way.
    -   中文: 以合适的方式可视化拟合的 QDA 模型的决策边界。
4.  **测试算法 (Test Algorithm):**
    -   English: Test the QDA algorithm on the input (200, 0, 200).
    -   中文: 在输入 (200, 0, 200) 上测试 QDA 算法。
5.  **预测颜色名称 (Predict Color Name):**
    -   English: Determine what color name is predicted by the algorithm for the input (200, 0, 200).
    -   中文: 确定算法对于输入 (200, 0, 200) 预测的颜色名称。
6.  **比较 kNN (Optional):**
    -   English: (Opportunity for extra effort) Learn about k-Nearest Neighbors (kNN) classification, implement it, and compare its performance with the QDA algorithm.
    -   中文: （展示额外努力的机会）了解 k 最近邻 (kNN) 分类，实现它，并将其性能与 QDA 算法进行比较。
