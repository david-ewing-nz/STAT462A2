---
title:    "STAT462 Assignment 2"
author: 
  -       "David Ewing (82171165)"
  -       "Xia Yu      (62380486)"
date:     "`r Sys.Date()`"

output:   
  pdf_document:
    latex_engine:     xelatex
    fig_caption:      true
    number_sections:  true
    toc:              true
    keep_tex:         true
    fig_crop:         false         # disable pdfcrop 
    includes:
      in_header: ../doc/fonts.tex
    
  mainfont: Arial
  fontsize: 10pt
---

```{r file-info, echo=FALSE, results='asis'}

cat("**R Markdown file name:**", knitr::current_input(), "\n\n")
cat("**R Markdown file name:**", rmarkdown::metadata$file, "\n")
cat("**Rendered on:**", format(Sys.time(), "%Y.%m.%d %H:%M:00"), "\n")
```

```{r setup, include=FALSE}
# This chunk was set up with the aid of ChatGPT.
# The intent is to load updates quietly thus not
# spending undue time with the logistics of getting 
# setup. 
 

#---------------- knitr::opts_chunk ---------------------------
#---------------- knitr::opts_chunk ---------------------------

knitr::opts_chunk$set(
  dev.args = list(
    png = list(type = "cairo")
    ),
  results = "markup",   # default("markup") "asis" "hide"	"hold"	"show"
  echo    = TRUE,      # Whether the code is shown
  eval    = TRUE,       # Whether the code is executed
  message = FALSE,      # Whether messages are printed
  warning = FALSE,     # Whether warnings are printed
  error   = FALSE       # Whether errors stop execution
  )


#---------------- loading libraries ---------------------------
#---------------- loading libraries ---------------------------

# Load configuration for reproducibility and preferred CRAN mirror
options(repos = c(CRAN = "https://cran.stat.auckland.ac.nz/"))

#library(conflicted) # before any other

# Required packages
required_packages <- c(
  "caret",         # Model training utilities
  "class",         # kNN 
  "cowplot",       # Plot composition
  "dplyr",         # Data wrangling
  "flextable",     # Summary tables
  "GGally",        # Pair plots
  "ggplot2",       # Core plotting
  "glmnet",        # Regularised regression
  "glue",          # glue
  "patchwork",      # ✅ for ggplot grid layouts
  "rsample",         # initial_split
  "kableExtra",    # Table formatting
  "knitr",         # Inline rendering
  "MASS",          # LDA/QDA and logistic
  "nnet",
  "officer",       # Word/PDF table styling (used by flextable)
  "skimr",         # Data summaries
  "tree",
  "tidyverse",     # Core data science packages
  "tibble"         # Modern data frames
)
for (pkg in required_packages) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
  }
  library(pkg, character.only = TRUE)
}

#---------------- conflict_prefer ------------------------------
#---------------- conflict_prefer ------------------------------
library(conflicted)
if ("conflicted" %in% loadedNamespaces()) {
  try(conflict_prefer("filter", "dplyr"), silent = TRUE)
  try(conflict_prefer("select", "dplyr"), silent = TRUE)
}

```


```{r helper-functions, include=FALSE}

#' Format a data frame as a styled flextable
pretty_df <- function(df,
                      title    = NULL,
                      fontsize = 10,
                      padding  = 5,  # defined here
                      n        = 5) {
  if (!is.data.frame(df)) {
    stop("Input must be a data frame")
  }

  # Optional row slicing
  if (n > 0) df <- head(df, n)
  if (n < 0) df <- tail(df, abs(n))

  ft <- flextable::flextable(df) |>
    flextable::fontsize(size = fontsize, part = "all") |>
    flextable::padding(padding = padding, part = "all") |>
    flextable::align(align = "center", part = "all") |>
    flextable::theme_booktabs() |>
    flextable::bold(i = 1, part = "header") |>
    flextable::autofit()

  # Estimate and optionally adjust for long titles
  if (!is.null(title)) {
    estimated_char_width <- 0.07
    title_width <- nchar(title) * estimated_char_width

    if (!is.null(ft$body$colwidths)) {
      current_width <- sum(ft$body$colwidths, na.rm = TRUE)

      if (title_width > current_width && current_width > 0) {
        scale_factor <- title_width / current_width
        new_widths <- ft$body$colwidths * scale_factor

        for (j in seq_along(new_widths)) {
          ft <- flextable::width(ft, j = j, width = new_widths[j])
        }
      }
    }

    ft <- flextable::set_caption(ft, caption = title)
  }

  return(ft)
}


#' Format a confusion matrix as flextable
pretty_cmtx <- function(cmtx_tbl, title = "Confusion Matrix") {
  df <- as.data.frame.matrix(cmtx_tbl)               # Convert to data.frame (wide format)
  df <- cbind(Predicted = rownames(df), df)      # Add row names as column
  rownames(df) <- NULL                           # Remove rownames
  pretty_df(df, title = title)                   # Format with your pretty_df()
}


#' Format classification metrics
pretty_heart_mtrx <- function(cmtx_tbl, title = "Classification Metrics") {
  metrics_df <- eval_heart_cmtx(cmtx_tbl)      # Compute the metrics
  pretty_df(metrics_df, title = title)      # Format as flextable
}



#' Summarise train/test splits
pretty_split <- function(train, test, target, title = "Train/Test Split Summary") {
  s <- function(data) {
    tbl <- table(data[[target]])
    out <- data.frame(
      Total = nrow(data),
      Class_1 = tbl[1],
      Class_2 = tbl[2],
      Prop_1 = round(tbl[1] / sum(tbl), 3),
      Prop_2 = round(tbl[2] / sum(tbl), 3)
    )
    rownames(out) <- NULL
    out
  }
  summary_df <- rbind(
    cbind(Set = "Train", s(train)),
    cbind(Set = "Test", s(test))
  )
  pretty_df(summary_df, title = title)
}


# ---- Pretty csv_read with Type Summary and Preview ------
pretty_read_csv <- \(path, n = 5, col_names = TRUE, show_col_types = FALSE) {
  df <- readr::read_csv(path, show_col_types = FALSE, col_names = col_names)
  df <- as.data.frame(df)
  ft <- pretty_df(df, glue::glue("CSV: {path}"))
  return(list(df = df, ft = ft))
}


# ---- Pretty Excel Reader with Type Summary and Preview ----
pretty_read_xlsx <- \(path, sheet = 1, col_names = TRUE, n = 0) {
  df <- readxl::read_excel(path, sheet = sheet, col_names = col_names)
  df <- as.data.frame(df)
  types <- purrr::map_chr(df, typeof)
  type_df <- data.frame(Column = names(df), Type = types, stringsAsFactors = FALSE)
  ft <- pretty_df(type_df, glue::glue("XLSX Column Types: {path}"))
  return(list(df = df, ft = ft))
}


# ---- Pretty ggplot metadata summary ----
pretty_ggplot <- function(plot, title = "ggplot Summary") {
  if (!inherits(plot, "gg")) stop("Input must be a ggplot object.")

  plot_data <- tryCatch(plot$data, error = function(e) NULL)
  geoms <- sapply(plot$layers, function(layer) class(layer$geom)[1])
  mappings <- plot$mapping

  global_aes  <- names(mappings)
  global_vals <- sapply(mappings, function(x) rlang::expr_text(x))

  p_title <- plot$labels$title %||% title %||% ""
  x_lab   <- plot$labels$x %||% ""
  y_lab   <- plot$labels$y %||% ""
  colour_lab <- plot$labels$colour %||% plot$labels$color %||% ""

  df <- data.frame(
    Component = c("Title", "X Axis", "Y Axis", "Colour Legend", "Geoms", global_aes),
    Value     = c(p_title, x_lab, y_lab, colour_lab, paste(geoms, collapse = ","), global_vals),
    stringsAsFactors = FALSE
  )
  pretty_df(df)
}


#' Pretty glm model summary
pretty_glm <- function(model, title = "GLM Model Summary" ) {
  if (!inherits(model, "glm")) {
    stop("pretty_glm() expects a glm object.")
  }
  tryCatch({
    df <- data.frame(
      Metric = c(
        "Formula", "AIC", "Null deviance", "Residual deviance",
        "Component names", "Model class"
      ),
      Value = c(
        deparse(formula(model)),
        AIC(model),
        model$null.deviance,
        model$deviance,
        paste(names(model), collapse = ", "),
        paste(class(model), collapse = ", ")
      ),
      stringsAsFactors = FALSE
    )
    ft <- pretty_df(df, title = "GLM Model Summary")
  }, error = function(e) {
    cat("Error in pretty_glm():", conditionMessage(e), "\n")
    NULL
  })
  return(ft)
}

pretty_multinom <- function(model, title = "Multinomial Model Summary") {
  if (!inherits(model, "multinom")) {
    stop("pretty_multinom() expects a multinom object.")
  }
#browser()
    #print(paste(deparse(formula(model)), collapse = ""))  
    #print(AIC(model))
    #print(model$converged)
  
    #Print(paste(class(model), collapse = ", ")) 
    #print(paste(names(model), collapse = ", "))
  tryCatch({
    # Summary table
    df_summary <- data.frame(
  Metric = c(
    "Formula", "AIC", "Converged", "Number of Iterations",
    "Model class", "Component names"
  ),
  Value = c(
    paste(deparse(formula(model)), collapse = ""),
    AIC(model),
    if (!is.null(model$converged)) model$converged else "N/A",
    if (!is.null(model$iters)) model$iters else "N/A",
    paste(class(model), collapse = ", "),
    paste(names(model), collapse = ", ")
  ),
  stringsAsFactors = FALSE
)


    ft_summary <- pretty_df(df_summary, title = title, n = nrow(df_summary))

    # Coefficients table
    coef_mat <- coef(model)
    coef_df <- as.data.frame(
      apply(coef_mat, 2, function(x) formatC(x, digits = 3, format = "e")),
      stringsAsFactors = FALSE
    )
    coef_df$Class <- rownames(coef_mat)
    coef_df <- coef_df[, c("Class", setdiff(names(coef_df), "Class"))]

    ft_coef <- pretty_df(coef_df, title = "Multinomial Model Coefficients")

    return(list(ft_summary, ft_coef))
  }, error = function(e) {
    cat("Error in pretty_multinom():", conditionMessage(e), "\n")
    NULL
  })
}



#' Pretty split df into multiple flextables
pretty_split_df <- function(df,
                            cols = 6,
                            title = NULL,
                            fontsize = 10,
                            n = 5) {
  if (!is.data.frame(df)) {
    stop(cat("\\textit{Object is not a data frame:}", deparse(df)))
  }

  title <- if (is.null(title)) deparse(substitute(df)) else title
  df_show <- if (n > 0) head(df, n) else if (n < 0) tail(df, abs(n)) else df
  col_groups <- split(names(df_show), ceiling(seq_along(df_show) / cols))

  ft_list <- lapply(seq_along(col_groups), function(i) {
    subdf <- df_show[, col_groups[[i]], drop = FALSE]
    pretty_df(
      subdf,
      title = paste0(title, " (", i, ")"),
      fontsize = fontsize,
      n = n
    )
  })

  names(ft_list) <- paste0(
    title,
    " (",
    seq_along(ft_list),
    ")"
  )

  return(ft_list)
}


#' Generate a full QDA summary with priors, centroids, and equations
pretty_qda <- function(qda_model) {
  response_var <- as.character(attr(qda_model$terms, "variables")[[2]])
  predictor_vars <- attr(qda_model$terms, "term.labels")

  qda_equation <- paste0(
    "P(", response_var, " = k | ", paste(predictor_vars, collapse = ", "),
    ") ∝ π_k × f_k(", paste(predictor_vars, collapse = ", "), ")"
  )

  centroids_df <- as.data.frame(qda_model$means)
  centroids_df[[response_var]] <- rownames(centroids_df)
  centroids_df <- centroids_df[, c(response_var, setdiff(names(centroids_df), response_var))]

  priors_df <- as.data.frame(qda_model$prior)
  priors_df[[response_var]] <- rownames(priors_df)
  colnames(priors_df) <- c("prior_probability", response_var)
  priors_df <- priors_df[, c(response_var, "prior_probability")]

  return(list(
    centroids = pretty_df(centroids_df,qda_equation),
    priors    = priors_df,pretty_df(priors_df,qda_equation),
    equation = qda_equation
  ))
}


#' Format classification thresholds summary
pretty_thresh <- function(predicted_probs, truth, threshold = 0.5, positive_class = NULL, title = "Threshold-Based Classification Summary") {
  predicted_class <- ifelse(predicted_probs >= threshold, positive_class, paste0("not_", positive_class))
  cm <- table(Truth = truth, Predicted = predicted_class)
  pretty_cmtx(cm, title = title)
}


#' Summarise variable transformations
pretty_transforms <- function(transform_map, title = "Variable Transformation Mapping") {
  df <- as.data.frame(transform_map)
  names(df) <- c("Original", "Transformed")
  pretty_df(df, title = title)
}


#' Summarise missing value patterns
pretty_missing <- function(df, title = "Missing Data Summary") {
  missings <- sapply(df, function(x) sum(is.na(x)))
  df_out <- data.frame(Variable = names(missings), Missing_Count = missings)
  df_out <- df_out[df_out$Missing_Count > 0, ]
  pretty_df(df_out, title = title)
}


#' Display model object summary (generic use for tree, lm, etc.)
pretty_model <- function(model, title = "Model Summary") {
  summary_text <- capture.output(summary(model))
  df <- data.frame(Summary = summary_text)
  pretty_df(df, title = title)
}


 


#' Pretty summary for numeric columns
pretty_summary <- function(df) {
  ft <- dplyr::select(df, where(is.numeric)) |>
    dplyr::summarise(
      dplyr::across(
        dplyr::everything(),
        .fns = list(
          Mean     = ~mean(.x, na.rm = TRUE),
          Median   = ~median(.x, na.rm = TRUE),
          Min      = ~min(.x, na.rm = TRUE),
          Max      = ~max(.x, na.rm = TRUE),
          IQR      = ~IQR(.x, na.rm = TRUE),
          nNA      = ~sum(is.na(.x))
        )
      )
    ) |>
    tidyr::pivot_longer(
      cols = dplyr::everything(),
      names_to = c("Variable", ".value"),
      names_sep = "_"
    ) |>
    dplyr::mutate(dplyr::across(where(is.numeric), round, 2)) |>
    flextable::flextable() |>
    flextable::set_header_labels(
      Variable = "Feature",
      Mean     = "Mean",
      Median   = "Median",
      Min      = "Min",
      Max      = "Max",
      IQR      = "Interquartile Range",
      nNA      = "Missing Values"
    ) |>
    flextable::autofit() |>
    flextable::theme_vanilla()

  ft
}

pretty_heart_comparison <- function(model_list) {
  stopifnot(is.list(model_list))
  
  # Storage
  comparison <- data.frame()
  
  for (name in names(model_list)) {
    entry <- model_list[[name]]
    
    # Required elements in each entry
    actual <- entry$actual
    predicted <- entry$predicted
    
    # Generate confusion matrix
    cm <- make_heart_cmtx(predicted, actual)
    metrics <- eval_heart_cmtx(cm)
    
    comparison <- rbind(comparison, data.frame(
      Model        = name,
      Accuracy     = round(metrics[2], 3),
      Precision    = round(metrics[3], 3),
      Recall       = round(metrics[4], 3),
      F1_Score     = round(metrics[5], 3),
      MisclassRate = round(metrics[1], 3)
    ))
  }
  
  # Reorder columns to preference
  comparison <- comparison[, c("Model", "Accuracy", "Precision", "Recall", "F1_Score", "MisclassRate")]
  
  # Return as pretty flextable
  pretty_df(comparison, title = "Model Performance Comparison")
  
  
  
}



```



```{r wsb-global-model-utils}
# --- Model Evaluation Utilities ---

model_boolean_eval <- function(model, pred_class, actual_class, train_data, threshold = 0.5) {
  # Ensure inputs are numeric 0/1
  actual_class <- if (is.factor(actual_class)) as.integer(actual_class) - 1 else actual_class
  pred_class   <- if (is.factor(pred_class)) as.integer(pred_class) - 1 else pred_class
  train_death  <- if (is.factor(train_data$DEATH)) as.integer(train_data$DEATH) - 1 else train_data$DEATH

  # Significance test
  pvals <- broom::tidy(model)$p.value
  sig_all <- all(pvals < 0.05)

  # Accuracy comparisons
  test_acc <- mean(pred_class == actual_class)
  base_acc <- max(prop.table(table(actual_class)))
  better_than_random <- test_acc > 0.5

  better_than_baseline_mild     <- (test_acc - base_acc) >= 0.01
  better_than_baseline_moderate <- (test_acc - base_acc) >= 0.015
  better_than_baseline_strong   <- (test_acc - base_acc) >= 0.02
  real_world_gain               <- (test_acc - base_acc) >= 0.05

  # Generalisation test
  train_probs <- predict(model, newdata = train_data, type = "response")
  train_class <- ifelse(train_probs > threshold, 1, 0)
  train_acc <- mean(train_class == train_death)
  well_generalised <- abs(train_acc - test_acc) < 0.05

  list(
    sig_all = sig_all,
    test_acc = test_acc,
    base_acc = base_acc,
    better_than_random = better_than_random,
    better_than_mild = better_than_baseline_mild,
    better_than_moderate = better_than_baseline_moderate,
    better_than_strong = better_than_baseline_strong,
    well_generalised = well_generalised,
    real_world_gain = real_world_gain
  )
}

model_multiclass_eval <- function(pred_class, actual_class) {
  # Ensure inputs are factors with correct levels
  labels <- c("RED", "GREEN", "BLUE")
  pred_class   <- if (is.numeric(pred_class)) factor(pred_class, levels = 1:3, labels = labels) else pred_class
  actual_class <- if (is.numeric(actual_class)) factor(actual_class, levels = 1:3, labels = labels) else actual_class

  # Accuracy and baseline
  accuracy <- mean(pred_class == actual_class)
  base_acc <- max(prop.table(table(actual_class)))
  better_than_baseline <- (accuracy - base_acc) >= 0.02
  strong_performance   <- accuracy >= 0.75

  # Macro-averaged precision and recall
  precision <- recall <- rep(NA, 3)
  for (i in seq_along(labels)) {
    class <- labels[i]
    tp <- sum(pred_class == class & actual_class == class)
    fp <- sum(pred_class == class & actual_class != class)
    fn <- sum(pred_class != class & actual_class == class)
    precision[i] <- if ((tp + fp) > 0) tp / (tp + fp) else NA
    recall[i]    <- if ((tp + fn) > 0) tp / (tp + fn) else NA
  }

  macro_precision <- mean(precision, na.rm = TRUE)
  macro_recall    <- mean(recall, na.rm = TRUE)

  list(
    accuracy = accuracy,
    base_acc = base_acc,
    better_than_baseline = better_than_baseline,
    strong_performance = strong_performance,
    macro_precision = macro_precision,
    macro_recall = macro_recall
  )
}

summarise_boolean_eval <- function(eval_list) {
  data.frame(
    Question = c(
      "Statistically significant?",
      "Better than random?",
      "Better than majority guess? (≥ 0.01)",
      "Better than majority guess? (≥ 0.015)",
      "Better than majority guess? (≥ 0.02)",
      "Well-generalised?",
      "Real-world useful?"
    ),
    Result = unlist(c(
      eval_list$sig_all,
      eval_list$better_than_random,
      eval_list$better_than_mild,
      eval_list$better_than_moderate,
      eval_list$better_than_strong,
      eval_list$well_generalised,
      eval_list$real_world_gain
    )),
    Justification = c(
      "all(pvals < 0.05)",
      "test_acc > 0.5",
      "(test_acc - base_acc) >= 0.01",
      "(test_acc - base_acc) >= 0.015",
      "(test_acc - base_acc) >= 0.02",
      "abs(train_acc - test_acc) < 0.05",
      "(test_acc - base_acc) >= 0.05"
    ),
    stringsAsFactors = FALSE
  )
}



#---------------- prediction & confusion helpers ------------------------------


predict_heart_classes <- function(probs, threshold = 0.5) {
  pred <- ifelse(probs > threshold, 1, 0)
  factor(pred, levels = c(0, 1), labels = c("LIVE", "DEAD")) #binary probabilities to factor labels for heart
}

# Create confusion matrix for heart
make_heart_cmtx <- function(pred_class, actual_class) {
  pred_class <- if (is.numeric(pred_class)) {
    factor(pred_class, levels = c(0, 1), labels = c("LIVE", "DEAD"))
  } else pred_class
  actual_class <- if (is.numeric(actual_class)) {
    factor(actual_class, levels = c(0, 1), labels = c("LIVE", "DEAD"))
  } else actual_class

  table(
    Predicted = factor(pred_class, levels = c("LIVE", "DEAD")),
    Actual    = factor(actual_class, levels = c("LIVE", "DEAD"))
  )
}

eval_heart_cmtx <- function(cmtx_tbl) {
  if (!(all(rownames(cmtx_tbl) == c("LIVE", "DEAD")) || !all(colnames(cmtx_tbl) == c("LIVE", "DEAD")))) {
    stop("Confusion matrix must use levels: LIVE and DEAD in [row, col] format")
  }

  tp <- cmtx_tbl["DEAD", "DEAD"]
  tn <- cmtx_tbl["LIVE", "LIVE"]
  fp <- cmtx_tbl["LIVE", "DEAD"]
  fn <- cmtx_tbl["DEAD", "LIVE"]
  total <- sum(cmtx_tbl)

  if (total == 0 || any(is.na(c(tp, tn, fp, fn)))) {
    return(data.frame(
      Metric = c("Misclass Rate", "Accuracy", "Precision", "Recall", "F1 Score"),
      Value = rep(NA_real_, 5)
    ))
  }

  accuracy  <- (tp + tn) / total
  precision <- if ((tp + fp) > 0) tp / (tp + fp) else NA
  recall    <- if ((tp + fn) > 0) tp / (tp + fn) else NA
  f1        <- if (!is.na(precision) && !is.na(recall) && (precision + recall) > 0) {
    2 * precision * recall / (precision + recall)
  } else NA
  misclass  <- 1 - accuracy

  data.frame(
    Metric = c("Misclass Rate", "Accuracy", "Precision", "Recall", "F1 Score"),
    Value  = round(c(misclass, accuracy, precision, recall, f1), 3)
  )
}



summarise_heart_results <- function(pred_probs, actual_class, threshold = 0.5) {
  pred_class <- predict_heart_classes(pred_probs, threshold)
  cmtx_tbl   <- make_heart_cmtx(pred_class, actual_class)
  metrics_df <- eval_heart_cmtx(cmtx_tbl)

  list( 
    pred_class = pred_class,
    cmtx_tbl   = cmtx_tbl,                     # raw
    metrics_df = metrics_df,                   # raw
    cmtx_ft    = pretty_cmtx(cmtx_tbl),
    mtrx_ft    = pretty_df(metrics_df)
  )
}




# Convert multiclass probabilities to factor labels for colour
predict_colour_classes <- function(probs) {
  pred <- apply(probs, 1, which.max)
  factor(pred, levels = 1:3, labels = c("RED", "GREEN", "BLUE"))
}

# Create confusion matrix for colour
make_colour_cmtx <- function(pred_class, actual_class) {
  pred_class <- if (is.numeric(pred_class)) factor(pred_class, levels = 1:3, labels = c("RED", "GREEN", "BLUE")) else pred_class
  actual_class <- if (is.numeric(actual_class)) factor(actual_class, levels = 1:3, labels = c("RED", "GREEN", "BLUE")) else actual_class
  table(
    Predicted = factor(pred_class, levels = c("RED", "GREEN", "BLUE")),
    Actual    = factor(actual_class, levels = c("RED", "GREEN", "BLUE"))
  )
}

eval_colour_cmtx <- function(cmtx) {
  expected_levels <- c("RED", "GREEN", "BLUE")
  if (!all(rownames(cmtx) == expected_levels) || !all(colnames(cmtx) == expected_levels)) {
    stop("Confusion matrix must use levels: RED, GREEN, BLUE in [row, col] format")
  }

  # Total correct predictions
  correct <- sum(diag(cmtx))
  total   <- sum(cmtx)
  accuracy <- correct / total

  # Precision, Recall, F1 per class
  precision_vec <- recall_vec <- f1_vec <- numeric(length(expected_levels))

  for (i in seq_along(expected_levels)) {
    cls <- expected_levels[i]
    tp  <- cmtx[cls, cls]
    fp  <- sum(cmtx[cls, ]) - tp
    fn  <- sum(cmtx[, cls]) - tp

    precision <- if ((tp + fp) > 0) tp / (tp + fp) else NA
    recall    <- if ((tp + fn) > 0) tp / (tp + fn) else NA
    f1        <- if (!is.na(precision) && !is.na(recall) && (precision + recall) > 0) {
                   2 * precision * recall / (precision + recall)
                 } else NA

    precision_vec[i] <- precision
    recall_vec[i]    <- recall
    f1_vec[i]        <- f1
  }

  macro_precision <- mean(precision_vec, na.rm = TRUE)
  macro_recall    <- mean(recall_vec, na.rm = TRUE)
  macro_f1        <- mean(f1_vec, na.rm = TRUE)
  misclass        <- 1 - accuracy

  data.frame(
    Metric = c("Misclass Rate", "Accuracy", "Precision (macro)", "Recall (macro)", "F1 Score (macro)"),
    Value  = round(c(misclass, accuracy, macro_precision, macro_recall, macro_f1), 3)
  )
}


make_colour_cmtx <- function(pred_class, actual_class) {
  pred_class <- if (is.numeric(pred_class)) {
    factor(pred_class, levels = 1:3, labels = c("RED", "GREEN", "BLUE"))
  } else pred_class
  actual_class <- if (is.numeric(actual_class)) {
    factor(actual_class, levels = 1:3, labels = c("RED", "GREEN", "BLUE"))
  } else actual_class

  table(
    Predicted = factor(pred_class, levels = c("RED", "GREEN", "BLUE")),
    Actual    = factor(actual_class, levels = c("RED", "GREEN", "BLUE"))
  )
}




```



```{r load-and-test-split, results='hide'}
# Load heart and colour datasets from zip

# Set base directory
# Assign named paths based on filename match
# load the dataframes via pretty_read_csv 
# preview structure
unzip_dir <- "../data/unzipped"
zip_path  <- "../data/data_assignment_2.zip"
csv_files <- unzip(zip_path, list = TRUE)$Name  # Extract filenames from the zip
target_paths <- file.path(unzip_dir, csv_files)

idx_color   <- grep("color", csv_files)
idx_heart   <- grep("heart", csv_files)
path_colour <- target_paths[idx_color]
path_heart  <- target_paths[idx_heart]
 
heart_list  <- pretty_read_csv(path_heart, col_names = TRUE)
color_ft <- pretty_read_csv(path_colour, col_names = TRUE)
   
heart_df = heart_list$df
heart_ft = heart_list$ft 
                                                         
color_df = color_ft$df
color_ft = color_ft$ft 


foo <- pretty_split_df(heart_df)

render_flextables <- function(ft_list) {
  for (ft in foo) {
    invisible(print(knitr::knit_print(ft)))
    }
  }

render_flextables(foo)
```
 
```{r wsb-step-03-split}
cat("### Split the dataset into a training set (80%) and a test set (20%)\n")

# Define conversion helpers
as_hp <- function(x) factor(x, levels = c(0, 1), labels = c("LIVE", "DEAD"))
as_h  <- function(x) as.integer(x) - 1

# Select and convert DEATH to binary early
h_vars   <- c("DEATH", "GLUCOSE", "SYSBP", "AGE")
h_clean  <- heart_df |>
  select(all_of(h_vars)) |>
  drop_na() |>
  mutate(DEATH = as_h(DEATH))  # 0/1 version for modeling

# Stratified split using binary DEATH
set.seed(82171165)
h_split <- initial_split(h_clean, prop = 0.8, strata = DEATH)
h_train <- training(h_split)
h_test  <- testing(h_split)

# Pretty/labeled version for visualization
hp_clean <- h_clean |> mutate(DEATH = as_hp(DEATH))
hp_train <- h_train |> mutate(DEATH = as_hp(DEATH))
hp_test  <- h_test  |> mutate(DEATH = as_hp(DEATH))

# True observed labels for evaluation
h_actual_class  <- h_test$DEATH
hp_actual_class <- hp_test$DEATH

```


```{r wsb-step-03-split}
cat("### Split the dataset into a training set (80%) and a test set (20%)
")

# heart_df exists at run time

h_vars <- c("DEATH", "GLUCOSE", "SYSBP", "AGE")
h_clean <- heart_df |>
  select(all_of(h_vars)) |>
  drop_na() |>
  mutate(DEATH = as.factor(DEATH))

# Stratified split : DEATH
set.seed(82171165)
h_split   <- initial_split(h_clean, prop = 0.8, strata = DEATH)
h_train   <- training(h_split)
h_test    <- testing(h_split)

# For visualisation only: relabel  "LIVE" and "DEAD"
levels = c(0, 1)
labels = c("LIVE", "DEAD")


hp_clean <- h_clean |>
  mutate(DEATH = factor(DEATH, levels = levels, labels = labels))
hp_train <- h_train |>
  mutate(DEATH = factor(DEATH, levels = levels, labels = labels))
hp_test <- h_test |>
  mutate(DEATH = factor(DEATH, levels = levels, labels = labels))

h_actual_class  <- h_test$DEATH        # true observed values
hp_actual_class <- hp_test$DEATH

```
### Visualise the relationship between DEATH, GLUCOSE and SYSBP
 
```{r wsb-step-04-visual}
cat("### Visualise the relationship between DEATH, GLUCOSE and SYSBP\n")

hp_relationship <- ggplot(hp_clean, aes(x = GLUCOSE, y = SYSBP, colour = DEATH)) +
  geom_point(alpha = 0.6) +
  labs(title = "DEATH vs GLUCOSE and SYSBP", x = "GLUCOSE", y = "SYSBP")

# 1. Distribution of SYSBP
hp_sysbp <- ggplot(hp_test, aes(x = SYSBP)) +
  geom_histogram(bins = 40, fill = "steelblue", colour = "white") +
  labs(title = "Distribution of SYSBP", x = "SYSBP", y = "Count")

# 2. Distribution of GLUCOSE
hp_glucose <- ggplot(hp_test, aes(x = GLUCOSE)) +
  geom_histogram(bins = 40, fill = "darkorange", colour = "white") +
  labs(title = "Distribution of GLUCOSE", x = "GLUCOSE", y = "Count")

# 3–6. Combination of linear/log axis scales
base <- ggplot(hp_test, aes(x = GLUCOSE, y = SYSBP)) +
  geom_point(alpha = 0.6) +
  labs(title = NULL, x = "GLUCOSE", y = "SYSBP")

hp_lin_lin <- base + ggtitle("Linear X / Linear Y")
hp_log_lin <- base + scale_x_log10() + ggtitle("Log X / Linear Y")
hp_lin_log <- base + scale_y_log10() + ggtitle("Linear X / Log Y")
hp_log_log <- base + scale_x_log10() + scale_y_log10() + ggtitle("Log X / Log Y")

# Combine 4 into a grid
hp_combined <- (hp_lin_lin | hp_log_lin) / (hp_lin_log | hp_log_log)

# Output all plots
print(hp_relationship)
print(hp_sysbp)
print(hp_glucose)
print(hp_combined)

pretty_ggplot(hp_relationship, title = "Relationship: DEATH vs GLUCOSE and SYSBP")
pretty_ggplot(hp_sysbp, title = "Distribution of SYSBP")
pretty_ggplot(hp_glucose, title = "Distribution of GLUCOSE")
pretty_ggplot(hp_lin_lin, title = "Linear X / Linear Y")
pretty_ggplot(hp_log_lin, title = "Log X / Linear Y")
pretty_ggplot(hp_lin_log, title = "Linear X / Log Y")
pretty_ggplot(hp_log_log, title = "Log X / Log Y")
pretty_ggplot(hp_combined, title = "Log/Linear Scale Comparison Grid")
```


### Form an initial hypothesis of what to look for when doing the classification

 ->
```{r  heart-wsb-step-03}
print("We hypothesise that high GLUCOSE and high SYSBP are associated with higher DEATH risk.")
```

### Fit a logistic regression model on the training set

 
```{r  heart-wsb-step-04}
               
equation_lg <- DEATH ~ GLUCOSE + SYSBP + AGE
hp_model <- glm(equation_lg, data = hp_train, family = "binomial")
pretty_glm(hp_model)


```


```{r wsb-step-05-model}
cat("### Fit a logistic regression model and generate predictions\n")

# 1. Fit logistic model and generate probabilities
h_equation_lg <- DEATH ~ GLUCOSE + SYSBP + AGE
h_model       <- glm(h_equation_lg, data = h_train, family = binomial)   # lrm on training
h_pred_probs  <- predict(h_model, newdata = h_test, type = "response")   # class probabilities on test
h_pred_class_50 <- predict_heart_classes(h_pred_probs, threshold = 0.5)  # prob -> class labels

# 2. Evaluate performance and format results
h_results_50 <- summarise_heart_results(h_pred_probs, hp_actual_class, threshold = 0.5)

# 3. Display formatted outputs
print(h_results_50$mtrx_df)     # classification metrics as flextable
print(h_results_50$cmtx_ft)     # confusion matrix as flextable

   
```

```{r wsb-step-05-eval-metrics-50}
cat("### Evaluate and display confusion matrix and metrics at threshold 0.5\n")

# 1. Predict class labels from probabilities
h_pred_probs <- predict(h_model, newdata = h_test, type = "response")
h_pred_class_50 <- predict_heart_classes(h_pred_probs, threshold = 0.5)


#h_metrics_50 <- eval_heart_cmtx(h_cmtx_50)
h_cmtx_50_tbl <- make_heart_cmtx(h_pred_class_50, hp_actual_class)
h_cmtx_50_ft <-  pretty_cmtx(h_cmtx_50_tbl, "Logistic Regression Confusion Matrix (Threshold = 0.5)") 
h_mtrx_50_df  <- eval_heart_cmtx(h_cmtx_50_tbl)
h_mtrx_50_ft <-  pretty_df(h_mtrx_50_df,"Logistic Regression Performance (Threshold = 0.5)") 

 
 
print(h_cmtx_50_ft)
print(h_mtrx_50_ft)
```





### Compute the misclassification rate on the test set

```{r wsb-step-06}
cat("### Compute the misclassification rate on the test set\n")

misclass_rate_50 <- h_mtrx_50_df$Value[h_mtrx_50_df$Metric == "Misclass Rate"]
# Print formatted summaries

```
 
  
### Comment on the goodness of fit
```{r  heart-wsb-step-08}
 
h_mtrx_50_df        # performance metrics
h_cmtx_50_tbl       # confusion matrix
hp_model            # the fitted GLM object

```

### Supporting numeric metrics for evaluation criteria

 
```{r heart-wsb-step-09}
cat("### Supporting numeric metrics for evaluation criteria\n")

# Predict class labels at threshold 0.1 using the standard helper
hp_pred_class_10 <- predict_heart_classes(h_pred_probs, threshold = 0.1)
```

### Recompute misclassification, confusion, and visualisation for 10% threshold
# Confusion Matrix (Threshold = 0.10)


 
```{r wsb-step-10}
cat("### Recompute misclassification, confusion, and visualisation for 10% threshold\n")

# Compute misclassification rate
hp_misclass_rate_10 <- mean(hp_pred_class_10 != hp_actual_class)

# Generate confusion matrix
hp_cmtx_10 <- make_heart_cmtx(hp_pred_class_10, hp_actual_class)

# Print for verification
cat("Misclassification Rate (10% threshold):", hp_misclass_rate_10, "\n")
print(hp_cmtx_10)
```
```{r heart-wsb-step-10.1}
### Compare logistic regression and discriminant analysis
### Evaluate Logistic Regression at Threshold 0.10
cat("### Logistic Regression Evaluation (Threshold = 0.10)\n")

# Reuse model, generate predicted probs
hp_pred_probs <- predict(hp_model, newdata = hp_test, type = "response")
hp_pred_class_10 <- predict_heart_classes(hp_pred_probs, threshold = 0.10)

# Actual class (assumed already defined earlier as hp_actual_class)
hp_cmtx_10_tbl <- make_heart_cmtx(hp_pred_class_10, hp_actual_class)
hp_mtrx_10_df   <- eval_heart_cmtx(hp_cmtx_10_tbl)

# Format and print results
hp_cmtx_10_ft <- pretty_cmtx(hp_cmtx_10_tbl, "Confusion Matrix (Threshold = 0.10)")
hp_mtrx_10_ft <- pretty_df(hp_mtrx_10_df, "Logistic Regression Performance (Threshold = 0.10)")

print(hp_cmtx_10_ft)
print(hp_mtrx_10_ft)

```


```




 
### QDA Evaluation
```{r heart-wsb-step-11}
cat("### QDA Evaluation\n")

hp_qda_model <- qda(equation_lg, data = hp_train) #QDA model using labeled data

# 2. Predict posterior probabilities and convert to class labels
hp_qda_pred_probs  <- predict(hp_qda_model, newdata = hp_test)$posterior[, "DEAD"]
hp_qda_pred_class_50  <- predict_heart_classes(hp_qda_pred_probs, threshold = 0.5)

# 3. Confusion matrix and performance metrics
hp_cmtx_qda_50   <- make_heart_cmtx(hp_qda_pred_class_50, hp_actual_class)
hp_metrics_qda_50 <- eval_heart_cmtx(hp_cmtx_qda_50)

# 4. Format both as flextables
ft_qda_cmtx_50   <- pretty_cmtx(hp_cmtx_qda_50, "QDA Confusion Matrix")
ft_qda_metrics_50 <- pretty_df(hp_metrics_qda_50, "QDA Model Performance")

# 5. Display
#print(ft_qda_cmtx_50)
#print(ft_qda_metrics)
```

 

### Visualise the decision boundaries of the fitted QDA model in a suitable way
 
```{r heart-wsb-step-12}
cat("### Visualise the decision boundaries of the fitted QDA model in a suitable way.\n")

# Predict posterior probabilities and classify with threshold 0.5
hp_qda_probs <- predict(hp_qda_model, newdata = hp_test)$posterior[, "DEAD"]
hp_qda_pred_class_50 <- predict_heart_classes(hp_qda_probs, threshold = 0.5)

# Plot QDA decision regions based on test data
ggp_qda_boundary_50 <- ggplot(hp_test, aes(x = GLUCOSE, y = SYSBP, colour = hp_qda_pred_class_50)) +
  geom_point(alpha = 0.5) +
  labs(title = "QDA Decision Boundary (Threshold = 0.5)", colour = "Predicted")

# Display
print(ggp_qda_boundary_50)
```

### Identify strong risk factors and communicate results


 
### Fit a tree-based model to the training set
 
```{r heart-wsb-step-13}
cat("### Fit a Tree-Based Model to the Training Set\n")

# 1. Fit classification tree to hp_train
hp_tree_model <- tree(equation_lg, data = hp_train)

# 2. Print model summary for verification
print(summary(hp_tree_model))

# 3. Visualise the tree structure
plot(hp_tree_model)
text(hp_tree_model, pretty = 0)
```

### Tree-Based or Optional Model Evaluation
```{r heart-wsb-step-13.2}
cat("### Tree-Based Model Evaluation (Threshold = 0.5)\n")

# 1. Predicted probabilities and classes
hp_tree_probs_50 <- predict(hp_tree_model, newdata = hp_test, type = "vector")
hp_tree_class_50 <- predict_heart_classes(hp_tree_probs_50[, "DEAD"], threshold = 0.5)

# 2. Confusion matrix and metrics
hp_cmtx_tree_50 <- make_heart_cmtx(hp_tree_class_50, hp_actual_class)
hp_metrics_tree_50 <- eval_heart_cmtx(hp_cmtx_tree_50)

# 3. Format for display
ft_tree_cmtx_50    <- pretty_cmtx(hp_cmtx_tree_50, "Tree-Based Model Confusion Matrix")
ft_tree_metrics_50 <- pretty_df(hp_metrics_tree_50, "Tree-Based Model Performance")

# 4. Display both tables
print(ft_tree_cmtx_50)
print(ft_tree_metrics_50)

```

```{r heart-wsb-step-14}

hp_pred_probs <- predict(hp_model, newdata = hp_test, type = "response")
hp_pred_glm_50   <- predict_heart_classes(hp_pred_probs, threshold = 0.5)

qda_output   <- predict(hp_qda_model, newdata = hp_test)
hp_pred_qda  <- predict_heart_classes(qda_output$posterior[, "DEAD"], threshold = 0.5)


hp_pred_probs_tree <- predict(hp_tree_model, newdata = hp_test, type = "vector")
hp_pred_tree <- predict_heart_classes(hp_pred_probs_tree[, "DEAD"], threshold = 0.5)

pretty_glm(hp_model)
pretty_qda(hp_qda_model)$equation
pretty_model_comparison(list(
  GLM  = list(actual = hp_actual_class, predicted = hp_pred_glm_50),
  QDA  = list(actual = hp_actual_class, predicted = hp_pred_qda),
  TREE = list(actual = hp_actual_class, predicted = hp_pred_tree)
))
```



### Step 1: Load and preview the Colour dataset
```{r colour-wsb-step-01-load} 
# Load heart and colour datasets from zip

print(color_ft)
```
 


### Step 2: Clean and prepare data
```{r colour-wsb-step-02-clean}
# Step 2–3: Clean and Split colour data

# Conversion helpers
as_cp <- function(x) factor(x, levels = c(1, 2, 3), labels = c("RED", "GREEN", "BLUE"))
as_c  <- function(x) as.integer(x)

# Clean and encode
c_vars   <- c("color", "r", "b")
c_clean  <- color_df |>
  select(all_of(c_vars)) |>
  drop_na() |>
  mutate(color = factor(color),         # ensure it's a factor first
         color = as_c(color))           # convert to integer for modeling

# Stratified split
set.seed(123)
c_split <- initial_split(c_clean, prop = 0.8, strata = color)
c_train <- training(c_split)
c_test  <- testing(c_split)

# Labeled version for display
cp_clean <- c_clean |> mutate(color = as_cp(color))
cp_train <- c_train |> mutate(color = as_cp(color))
cp_test  <- c_test  |> mutate(color = as_cp(color))

# For evaluation
cp_actual_class <- cp_test$color
```

```{r colour-wsb-step-04-visualise}
# Step 4: Visualise class distribution and feature relationships using cp_*

cp_plot_dist <- ggplot(cp_clean, aes(x = r, y = b, color = color)) +
  geom_point(alpha = 0.6) +
  labs(title = "Colour Class Distribution by R and B")

cp_plot_train <- ggplot(cp_train, aes(x = r, y = b, color = color)) +
  geom_point(alpha = 0.6) +
  labs(title = "Colour Training Set")

cp_plot_test <- ggplot(cp_test, aes(x = r, y = b, color = color)) +
  geom_point(alpha = 0.6) +
  labs(title = "Colour Test Set")

print(pretty_ggplot(cp_plot_dist, title = "Class Distribution: r vs b"))
print(pretty_ggplot(cp_plot_train, title = "Training Set: r vs b"))
print(pretty_ggplot(cp_plot_test, title = "Test Set: r vs b"))
```



### Step 5: Fit logistic regression model
```{r colour-wsb-step-05-model}
c_equation_lg = color ~ r + b
c_model <- multinom(c_equation_lg, data = c_train, family = "multinomial")
list_of_df <- pretty_multinom(c_model)

if (!is.null(list_of_df)) {
  for (ft in list_of_df) {
    print(ft)
  }
}



```


### Step 6: Evaluate logistic model on test set
```{r colour-wsb-step-06-eval}

c_actual_class <- c_test$color

c_pred_glm <- predict(c_model, newdata = c_test, type = "class")
c_cmtx_glm <- make_heart_cmtx(c_pred_glm, c_actual_class)
c_metrics_glm <- eval_heart_cmtx(c_cmtx_glm)
c_mtrx_df_glm <- summarise_metrics(c_metrics_glm)

print(pretty_cm(c_cmtx_glm, title = "GLM Confusion Matrix"))
print(pretty_df(c_mtrx_df_glm, title = "GLM Performance Metrics"))
```


### Step 7: Fit QDA model
```{r colour-wsb-step-07-qda-model}
c_qda_model <- qda(c_equation_lg, data = c_train)
qda_info <- pretty_qda(c_qda_model)
print(qda_info)
 
#print(pretty_df(qda_info$centroids, title = "QDA Class Centroids"))
#print(pretty_df(qda_info$priors, title = "QDA Prior Probabilities"))
cat("Symbolic QDA Equation:\n", qda_info$equation)
```


### Step 8: Evaluate QDA model
```{r colour-wsb-step-08-qda-eval}
c_qda_probs <- predict(c_qda_model, newdata = c_test)$posterior
c_pred_qda  <- colnames(c_qda_probs)[apply(c_qda_probs, 1, which.max)]
c_pred_qda  <- factor(c_pred_qda, levels = levels(c_actual_class))

c_cmtx_qda <- make_conf_matrx(c_pred_qda, c_actual_class)
c_metrics_qda <- eval_heart_cmtx(c_cmtx_qda)
c_mtrx_df_qda <- summarise_metrics(c_metrics_qda)

print(pretty_cm(c_cmtx_qda, title = "QDA Confusion Matrix"))
print(pretty_df(c_mtrx_df_qda, title = "QDA Performance Metrics"))
```


### Step 9: Predict Colour(200, 0, 200)
```{r colour-wsb-step-09-predictRGB}
new_colour <- data.frame(r = 200, b = 0)

# GLM prediction
glm_class <- predict(c_model, newdata = new_colour, type = "class")

# QDA prediction
qda_probs <- predict(c_qda_model, newdata = new_colour)$posterior
qda_class <- colnames(qda_probs)[which.max(qda_probs)]

cat("Prediction for color(200, 0, 200):\n")
cat("GLM: ", glm_class, "\n")
cat("QDA: ", qda_class, "\n")
```


### Step 10: Compare model performance
```{r colour-wsb-step-10-compare}
print(pretty_model_comparison(list(
  GLM  = list(actual = c_actual_class, predicted = c_pred_glm),
  QDA  = list(actual = c_actual_class, predicted = c_pred_qda)
)))
```


### Step 11: Summary and discussion
```{r colour-wsb-step-11-summary}
cat("\nModel Summary Notes:\n")
cat("- Logistic regression worked well for separating linear boundaries.\n")
cat("- QDA captured more complex class regions based on priors and covariances.\n")
cat("- Predictions for Colour(200, 0, 200) showed some model disagreement.\n")



```



```


c_train <- 1
## Introduction

This report presents a comprehensive statistical modeling analysis conducted for Assignment 2 of STAT462. The objective is to develop and evaluate predictive models for a multi-class classification problem using color data. The response variable of interest is `colour`, which consists of five categories: red, blue, pink, purple, and brown. The predictor variables are RGB color values (`r` and `b`), extracted from images.

Two modeling approaches are explored and compared:

1. **Quadratic Discriminant Analysis (QDA)** — a generative classification model that fits a separate multivariate normal distribution for each class and allows each class to have its own covariance matrix.

2. **k-Nearest Neighbors (k-NN)** — a non-parametric method that classifies a point based on the majority class among its k nearest neighbors in the feature space.

Throughout the report, these models are evaluated based on their classification accuracy, decision boundaries, and ROC curves (using one-vs-rest strategy) to assess their ability to distinguish each class from the others.

The analysis proceeds by first introducing the dataset, followed by model development, performance evaluation, and interpretation of results.


# 2. Data

The dataset used in this analysis contains RGB color values (`r`, `g`, `b`) extracted from image samples, with an associated color label stored in the `colour` variable. The objective is to classify these color labels based on the numeric RGB values.

After loading, the dataset was split into training and testing sets. The training set is used to build the models, while the testing set is used for performance evaluation.
```{r}

```

```{r data-class-distribution, echo=FALSE, message=FALSE, warning=FALSE}
# Generate class summary from the training data
#c_class_summary <- c_train %>%
 # count(colour, name = "Count") %>%
#  mutate(Proportion = round(Count / sum(Count), 3))

# Display formatted table
#pretty_df(c_class_summary, title = "Class Distribution in Training Set")
```

This table (Figure \@ref(fig:class-distribution)) shows the number and proportion of observations for each color class in the training set.
le above shows how many samples belong to each color class and what proportion they represent in the training data. This helps assess class balance and potential modeling challenges.


# Describe the dataset, variables, and preparation
```

# 3. Methodology

```{r methodology}
# Describe QDA and k-NN, tuning, training approach
```

# 4. Model Evaluation

```{r evaluation}
# Accuracy, ROC curves, confusion matrices
```

# 5. Model Comparison

```{r comparison}
# Compare QDA vs kNN using metrics and visuals
```

# 6. Discussion

```{r discussion}
# Interpret the findings, implications
```

# 7. Conclusion

```{r conclusion}
# Summarize the report, restate findings, recommendations
```

# References

```{r references, include=FALSE}
# BibTeX or manual citation list
```

# Appendix

```{r appendix}
# Additional plots or tables
```
